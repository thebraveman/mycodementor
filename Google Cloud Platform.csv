published;title;summary;link
Mon, 07 Dec 2020 19:00:00 -0000;Pub/Sub makes scalable real-time analytics more accessible than ever;<div class="block-paragraph"><div class="rich-text"><p>These days, real-time analytics has become critical for business. Automated, real-time decisions based on up-to-the-second data are no longer just for advanced, tech-first companies. It is becoming a basic way of doing business. <a href="https://www.zdnet.com/article/by-2025-nearly-30-percent-of-data-generated-will-be-real-time-idc-says" target="_blank">According to IDC</a>, more than a quarter of data created will be real-time in the next five years. The factors we see driving this growth are the competitive pressure to improve service and user experience quality. Another factor is the consumerization of many traditional businesses where many functions that used to be performed by agents are now done by consumers themselves. Now, every bank, retailer, and service provider needs to have a number of user interfaces, from internal apps, to mobile apps, and web apps. These interfaces not only require fresh data to operate but also produce transaction and interaction data at unprecedented scale. </p><p>Real-time data is not just about application features. It is fundamentally about scaling operations to deliver great user experiences: up-to-date systems monitoring, alerts, customer service dashboards, and automated controls for anything from industrial machinery to customer service operations to consumer devices. It can accelerate data insights to action and in turn increase operational responsiveness.</p><p>“With Google Cloud, we’ve been able to build a truly real-time engagement platform,” says Levente Otti, Head of Data, <a href="https://emarsys.com/" target="_blank">Emarsys</a>. “The norm used to be daily batch processing of data. Now, if an event happens, marketing actions can be executed within seconds, and customers can react immediately. That makes us very competitive in our market.” </p><h3>Real-time analytics all starts with messaging </h3><p>At Google, we’ve contended with the challenge of creating real-time user experiences at a vast scale from the early days of the company. A key component of our solution for this is Pub/Sub, a global, horizontally scalable messaging system. For over a decade, Google products, including Ads, Search and Gmail, have been using this infrastructure to handle <a href="https://cloud.google.com/pubsub/architecture">hundreds of millions of events per second</a>. </p><p>Several years ago, we made this system available to the world as <a href="https://cloud.google.com/pubsub/architecture">Cloud Pub/Sub</a>. Pub/Sub is uniquely easy to use. Traditional messaging middleware offered many of the same features, but were not designed to scale horizontally or were offered as services. Apache Kafka, the open-source stream processing platform, has solved the scalability problem by creating a distributed, partitioned log that supported horizontally scalable streaming writes and reads. Managed services inspired by the same idea have sprung up. Because these services are generally based on the notion of a fixed, local resource, such as a partition or a cluster, these services still left the users to solve the problem of global distribution of data and managing capacity.</p><p>Pub/Sub took automated capacity management to an extreme: Data producers need not worry about the capacity required to deliver data to subscribers, with up to 10,000 subscritions per topic supported. In fact, consumers even pay for the capacity needed to read the data independently from the data producers. The global nature of Pub/Sub is unique, with a single endpoint resolving to nearby regions for fast persistence of data. On the other side, the subscribers can be anywhere and receive a single stream of data aggregated from across all regions. At the same time, users retain precise control over where the data is stored and how it makes it there. This makes Pub/Sub a convenient way to make data available to a broad range of applications on Google Cloud and elsewhere, from ingestion into BigQuery to automated, real-time AI-assisted decision making with Dataflow. This provides data practitioners with the choice of creating an integrated feedback loop easily. "Our clients around the world increasingly are looking for quality real-time data within the cloud," said Trey Berre, <a href="https://www.cmegroup.com/" target="_blank">CME Group</a> Global Head of Data Services. "This innovative collaboration with Google Cloud will not only make it easier for our clients to access the data they need from anywhere with an internet connection, but will also make it easier than ever to integrate our market data into new cloud-based technologies." </p><h3>Making messaging more accessible</h3><p>In 2020, we have focused on making Pub/Sub even simpler. We observed that some of our users had to adapt their application design to the guarantees made by the service. Others were left building their own cost-optimized Apache Kafka clusters to achieve ultra low-cost targets. To address these pain points, we have made Pub/Sub much easier to use for several use cases and introduced an offering that achieves an order of magnitude lower total cost of ownership (TCO) for our customers. </p><h3>The cost-efficient ingestion option</h3><p>We set out to build a version of Pub/Sub for customers who needed a horizontally scalable messaging service at a cost typical of cost-optimized, self-managed single-zone Apache Kafka or similar OSS systems. The result is Pub/Sub Lite, which can match or even improve upon the TCO of running your own OSS solution. In comparison to Pub/Sub itself, Pub/Sub Lite is as much as <a href="https://cloud.google.com/pubsub/lite/pricing#comparing_pricing">ten times cheaper</a>, as long as the single-zone availability and capacity management models work for your use case. This managed service is suitable for a number of use cases, including:</p><ul><li><p><b>Security log analysis</b>, where it is often a cost center and not every event must be scanned to detect threats </p></li><li><p><b>Search indexes and serving cache updates</b>, which are commonly “best effort” cost-saving measures and don’t require a highly reliable messaging service</p></li><li><p><b>Gaming and media behavior analytics</b>, where low price is often key to getting startups off the ground</p></li></ul><p>This<a href="https://cloud.google.com/pubsub/docs/choosing-pubsub-or-lite">guide to choosing between Pub/Sub and Pub/Sub Lite</a> and the<a href="https://cloud.google.com/pubsub/lite/pricing#comparing_pricing">pricing comparisons</a> can help you decide if Lite is for you. </p><h3>Comprehensive and enterprise-ready messaging that scales</h3><p>This year, Pub/Sub added a number of features that will allow our users to simplify their code significantly. These features include: </p><ul><li><p><a href="https://cloud.google.com/pubsub/docs/ordering"><b>Scalable message ordering</b></a>: Scalable message delivery in order is a tough problem and critical for many applications, from general change data capture (CDC) to airplane operations. We were able to make this work with only minimal changes to our APIs and without sacrificing scalability and the on-demand capacity. Your applications that require ordering can now be much less stateful, and thus simpler to write and operate. There are no shards or partitions and every message for a key, such as a customer ID, arrives in order reliably. </p></li><li><p><a href="https://cloud.google.com/pubsub/docs/dead-letter-topics"><b>Dead-letter topics</b></a> automatically detect messages that repeatedly cause applications to fail and put them aside for manual, off-line debugging. This saves on processing time and keeps processing pipeline latency low. </p></li><li><p><a href="https://cloud.google.com/pubsub/docs/filtering"><b>Filters</b></a> automatically drop messages your application does not care to receive, saving on processing and egress costs. Filters are configuration, so there is no need to write code or deploy an application. It’s that simple. </p></li><li><p><b>Data residency controls</b>: In addition to Pub/Sub’s <a href="https://cloud.google.com/pubsub/docs/resource-location-restriction">resource location constraints</a>, which allows organizations to dictate where Pub/Sub stores message data regardless of where it is published, we have launched <a href="https://cloud.google.com/pubsub/docs/reference/service_apis_overview#service_endpoints">regional endpoints</a> to give you a way of connecting to Pub/Sub servers in a specific region. </p></li><li><p><b>Publisher flow control</b> (<a href="https://googleapis.dev/java/gax/1.60.0/com/google/api/gax/batching/BatchingSettings.html" target="_blank">Java</a>, <a href="https://googleapis.dev/python/pubsub/latest/publisher/index.html#publish-flow-control" target="_blank">Python</a>) is perhaps the most notable of many updates to our client libraries. Flow control is another surprisingly tough problem, as many applications require multiple threads to publish data concurrently, which can overwhelm the client machine’s network stack and lose data unless the threads coordinate. With flow control, you can achieve very high, sustainable publish rates safely. </p></li><li><p>Also of note are <a href="https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions">configurable retry policy</a> and <a href="https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/detach">subscription detachment</a>. </p></li></ul><p>As one of our users recently said: “I’m going to go and use this right now.”</p><h3>What’s next</h3>We will continue to make Pub/Sub and our real-time processing tools easier to use in the coming months. You can stay up-to-date by watching our <a href="https://cloud.google.com/pubsub/docs/release-notes">release notes</a>. In the meantime, we invite you to learn more about how to get started and everything you can do with Google Cloud’s real-time stream analytics services in our <a href="https://cloud.google.com/pubsub/docs/overview">documentation</a> or by contacting the <a href="https://cloud.google.com/contact">Google Cloud sales team</a>.</div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/data-analytics/try-spotifys-internal-os-tool-for-media-processing-in-beam/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Simplify creating data pipelines for media with Spotify’s Klio</h4><p class="uni-related-article-tout__body">Spotify open-sources Klio: scalable, efficient media processing on top of Apache Beam.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/products/data-analytics/move-from-batch-to-stream-processing-for-real-time-analytics-with-pub-sub/
Mon, 07 Dec 2020 18:30:00 -0000;Enabling Microsoft-based workloads with file storage options on Google Cloud;<div class="block-paragraph"><div class="rich-text"><p>Enterprises are rapidly moving <a href="https://www.youtube.com/watch?v=fqTC45HL6J4" target="_blank">Microsoft and Windows-based workloads to the cloud</a> to reduce license spend and embark on modernization strategies to fully leverage the power of cloud-native architecture. Today’s business climate requires agility, elasticity, scale, and cost optimization, all of which are far more difficult to attain by operating out of data centers. <a href="https://cloud.google.com/">Google Cloud</a> offers <a href="https://cloud.google.com/windows">a top-level enterprise-grade experience</a> for Microsoft-based services and tools. </p><p>Many Windows-based workloads require a Server Message Block (SMB) file service component. For example, <a href="https://blogs.sap.com/2017/07/21/how-to-create-a-high-available-sapmnt-share/" target="_blank">highly available SAP application servers running in Windows Server clusters need SMB file servers</a> to store configuration files and logs centrally. The COVID-19 pandemic has resulted in increased demand for <a href="https://cloud.google.com/solutions/virtual-desktops">virtual desktop solutions</a> to enable workers to adapt to the sudden necessity of working remotely. Those virtual desktop users often require access to SMB file servers to store documents and to collaborate with coworkers. </p><p>Fortunately, there are numerous options for SMB file services in Google Cloud that meet the varying needs of Microsoft shops. They fall into three categories: fully managed, semi-managed, and self-managed services. In this post, we’ll examine several options across those three buckets. (Note: this is by no means an exhaustive list of SMB file service providers for Google Cloud. Rather, this is a brief review of some of the common ones.)</p><h3>Fully managed SMB file services</h3><p>For many enterprises, reducing operational overhead is a key objective of their cloud transformation. Fully managed services provide the capabilities and outcomes, without requiring IT staff to worry about mundane tasks like software installation and configuration, application patching, and backup. These managed SMB file service options let customers get their Windows applications and users to work expeditiously, reducing toil and risk. (Note that these are managed partner-provided services, so make sure to check the region you’ll be using to ensure availability.)</p><p><b>NetApp Cloud Volumes Service</b></p><p>If you work in IT and have ever managed, used, or thought about storage, chances are you’re familiar with NetApp. NetApp has been providing enterprise-grade solutions since 1992. With <a href="https://cloud.google.com/netapp">NetApp Cloud Volumes Service (CVS)</a>, you get highly available, cloud-native, managed SMB services that are well-integrated with Google Cloud. Storage volumes can be sized from 1 to 100 TB to meet the demands of large-scale application environments, and the service includes tried-and-true NetApp features like automated snapshots and rapid volume provisioning. It can be <a href="https://console.cloud.google.com/marketplace/product/endpoints/cloudvolumesgcp-api.netapp.com">deployed right from the Google Cloud Marketplace</a>, managed in the Google Cloud console, supported by Google, and paid for in your Google Cloud bill.</p><p><b>Dell Technologies PowerScale</b></p><p>Dell Technologies is another leader in the enterprise storage market, and have partnered with them to offer <a href="https://www.delltechnologies.com/en-us/cloud/cloud-services/storage/powerscale-for-google-cloud.htm" target="_blank">PowerScale on Google Cloud</a>. PowerScale leverages an all-flash architecture for blazing fast storage operations. However, it will be backward-compatible, allowing you to choose between PowerScale all-flash nodes and Isilon nodes in all-flash, hybrid, or archive configuration. The OneFS file system boasts a maximum of 50 PB per namespace this thing scales! And as with NetApp, PowerScale in Google Cloud includes enterprise-grade features like snapshots, replication, and hybrid integration with on-premises storage. It’s tightly integrated with Google Cloud: it can be found <a href="https://console.cloud.google.com/marketplace/details/cloudonefs/onefs-public-listing">in the Google Cloud Marketplace</a>, is integrated with the Google Cloud console, and billed and supported directly by Google.</p><p>Both of these managed file storage products support up to SMBv3, making them outstanding options to support Windows workloads, without a lot of management overhead.  </p><h3>Semi-managed SMB file services</h3><p>Not everyone wants fully managed SMB services. While managed services take a lot of work off your plate, as a general rule they also reduce the ways in which you can customize the solution to meet your particular requirements. Therefore, some customers prefer to use self-managed (or semi-managed) services, like the storage services below, to tailor the configurations to the exact specifications needed for their Windows workloads.</p><p><b>NetApp Cloud Volumes ONTAP</b></p><p>Like the fully managed NetApp Cloud Volumes Service, <a href="https://cloud.netapp.com/ontap-cloud" target="_blank">NetApp Cloud Volumes ONTAP (CVO)</a> gives you the familiar features and benefits you’re likely used to with NetApp in your data center, including SnapMirror. However, as a semi-managed service, it’s well-suited for customers who need enhanced control and security of their data on Google Cloud. CVO deploys into your <a href="https://cloud.google.com/vpc">Google Cloud virtual private cloud</a> (VPC) on <a href="https://cloud.google.com/compute">Google Compute Engine</a> instances, all within your own Google Cloud project(s), so you can enforce policies, firewall rules, and user access as you see fit to meet internal or external compliance requirements. You will need to deploy CVO yourself by following NetApp’s <a href="https://docs.netapp.com/us-en/occm/task_getting_started_gcp.html" target="_blank">step-by-step instructions</a>. In the <a href="https://console.cloud.google.com/marketplace/details/netapp-cloudmanager/cloud-manager">Marketplace</a>, you get your choice of a number of CVO price plans, each with varying SMB storage capacity (2 TB to 368 TB) and availability. NetApp Cloud Volumes ONTAP is available in all Google Cloud regions.</p><p><b>Panzura Freedom Hybrid Cloud Storage</b></p><p><a href="https://cloud.google.com/solutions/partners/panzura-hybrid-cloud-storage">Panzura Freedom</a> is a born-in-the-cloud, hybrid file service that allows global enterprises to store, collaborate, and back up files. It presents a single, geo-distributed file system called Panzura CloudFS that’s simultaneously accessible from your Google Cloud VPCs, corporate offices, on-premises data centers, and other clouds. The authoritative data is stored in <a href="https://cloud.google.com/storage">Google Cloud Storage</a> buckets and cached in Panzura Freedom Filers deployed locally, giving your Windows applications and users high-performing access to the file system. <a href="https://cloud.withgoogle.com/infrastructure/explore" target="_blank">Google Cloud’s global fiber network</a> and 100+ points of presence (PoPs) reduce global latency to ensure fast access from anywhere. <a href="https://console.cloud.google.com/marketplace/browse?q=panzura">Panzura can be found in the Google Cloud Marketplace</a> as well.  </p><h3>Self-managed SMB file services</h3><p>In some cases, managed services will not meet all the requirements. This is not limited to technical requirements. For example, in your industry you might be subject to a compliance regulation for which none of the managed services are certified. If you consider all of the fully managed and semi-managed SMB file service options, but none of them are just right for your budget and requirements, don’t worry. You still have the option of rolling your own Windows SMB file service on Google Cloud. This approach gives you the most flexibility of all, along with the responsibility of deploying, configuring, securing, and managing it all. Don’t let that scare you, though: These options are likely very familiar to your Microsoft-focused staff.</p><p><b>Windows SMB file servers on a Google Compute Engine instance</b></p><p>This option is quite simple: you deploy a <a href="https://cloud.google.com/compute">Compute Engine</a> instance running your preferred version of Windows Server, install the File Server role, and you’re off to the races. You’ll have all the native features of Windows at your disposal. If you’ve <a href="https://cloud.google.com/solutions/patterns-for-using-active-directory-in-a-hybrid-environment">extended</a> or <a href="https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction">federated</a> your on-premises Active Directory into Google Cloud or are using the <a href="https://cloud.google.com/managed-microsoft-ad">Managed Service for Active Directory</a>, you’ll be able to apply permissions just as you do on-prem.  <a href="https://cloud.google.com/persistent-disk">Persistent Disks</a> add a great deal of flexibility to Windows file servers. You can add or expand Persistent Disks to increase the storage capacity and disk performance of your SMB file servers with no downtime. Although a single SMB file server is a single point of failure, the native protections and redundancies of Compute Engine make it unlikely that a failure will result in extended downtime. If you choose to utilize <a href="https://cloud.google.com/compute/docs/disks#repds">Regional Persistent Disks</a>, your disks will be continuously replicated to a different Google Cloud zone, adding an additional measure of protection and rapid recoverability in the event of a VM or zone failure.  </p><p><b>Windows clustering</b></p><p>If your requirements dictate that your Windows file services cannot go down, a single Windows file server will not do. Fortunately, there’s a solution: <a href="https://docs.microsoft.com/en-us/windows-server/failover-clustering/failover-clustering-overview" target="_blank">Windows Failover Clustering</a>. With two or more Windows Compute Engine instances and Persistent Disks, you can build a highly available SMB file cluster that can survive the failure of Persistent Disks, VMs, the OS, or even a whole Google Cloud zone with little or no downtime. There are two different flavors of Windows file clusters: File Server Cluster and Scale-out File server (SOFS).  </p><p><a href="https://cloud.google.com/compute/docs/tutorials/running-windows-server-failover-clustering"><b>Windows file server clusters</b></a> have been around for around 20 years. The basic architecture is two Windows servers in a Windows Failover Cluster, connected to shared storage such as a storage area network (SAN). These clusters are active-passive in nature. At any given time, only one of the servers in the cluster can access the shared storage and provide file services to SMB clients. Clients access the services via a floating IP address, front-ended by an <a href="https://cloud.google.com/load-balancing/docs/internal">internal load balancer</a>. In the event of a failure of the active node, the passive node will establish read/write access to the shared storage, bind the floating IP address, and launch file services. In a cloud environment, physical shared storage devices cannot be used for cluster storage. Instead, <a href="https://docs.microsoft.com/en-us/windows-server/storage/storage-spaces/storage-spaces-direct-overview" target="_blank">Storage Spaces Direct (S2D)</a> may be used. S2D is a clustered storage system that combines the persistent disks of multiple VMs into a single, highly available, virtual storage pool. You can think of it as a distributed virtual SAN.</p><p><a href="https://docs.microsoft.com/en-us/windows-server/failover-clustering/sofs-overview" target="_blank"><b>Scale-Out File Server (SOFS)</b></a> is a newer and more capable clustered file service role that also runs in a Windows Failover Cluster. Like Windows File Server Clusters, SOFS makes use of S2D for cluster storage. Unlike a Windows File Server Cluster, SOFS is an active-active file server. Rather than presenting a floating IP address to clients, SOFS creates separate A records in DNS for each node in the SOFS role. Each node has a complete replica of the shared dataset and can serve files to Windows clients, making SOFS both vertically and horizontally scalable. Additionally, <a href="https://docs.microsoft.com/en-us/windows-server/failover-clustering/sofs-overview#when-to-use-scale-out-file-server" target="_blank">SOFS has some newer features</a> that make it more resilient for application servers.  </p><p>As mentioned before, both Windows File Server Clusters and SOFS depend on S2D for shared storage. You can see the process of <a href="https://cloud.google.com/compute/docs/tutorials/running-windows-server-failover-clustering">installing S2D on Google Cloud virtual machines here</a>is described, and the chosen SMB file service role may be installed afterwards. Check out the process of <a href="https://docs.microsoft.com/en-us/windows-server/failover-clustering/deploy-two-node-clustered-file-server" target="_blank">deploying a file server cluster role here</a>, and <a href="https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/hh831718%28v%3dws.11%29" target="_blank">the process for an SOFS role</a>.  </p><p><b>Scale-Out File Server or File Server Cluster?</b></p><p>File Server Clusters and SOFS are alike in that they provide highly available SMB file shares on S2D. SOFS is a newer technology that provides higher throughput and more scalability than File Server Cluster. However, SOFS is not optimized for the metadata-heavy operations common with end-user file utilization (opening, renaming, editing, copying, etc.). Therefore, in general, choose File Server Clusters for end-user file services and choose SOFS when your application(s) need SMB file services. See <a href="https://docs.microsoft.com/en-us/windows-server/failover-clustering/sofs-overview#when-to-use-scale-out-file-server" target="_blank">this page</a> for a detailed comparison of features between File Server Cluster (referred to there as “General Use File Server Cluster”) and SOFS.</p><h3>Which option should I choose?</h3><p>We’ve described several good options for Microsoft shops to provide their Windows workloads and users access to secure, high-performing, and scalable SMB file services. How do you choose which one is best suited for your particular needs? Here are some decision criteria you should consider:</p><ul><li><p>Are you looking to simplify your IT operations and offload operational toil? If so, look at the fully managed and semi-managed options.</p></li><li><p>Do you have specialized technical configuration requirements that aren’t met by a managed service? Then consider rolling your own SMB file service solution as a single Windows instance or one of the Windows cluster options.</p></li><li><p>Do you require a multi-zone for fully automated high availability? If so, NetApp Cloud Volumes ONTAP and the single instance Windows file server are off the table. They run in a <a href="https://cloud.google.com/compute/docs/regions-zones">single Google Cloud zone</a>.</p></li><li><p>Do you have a requirement for a particular Google Cloud region? If so, you’ll need to verify whether NetApp Cloud Volumes Service and NetApp Cloud Volumes ONTAP are available in the region you require. As partner services that require specialized hardware, these two services are available in many, but not all, Google Cloud regions today.</p></li><li><p>Do you require hybrid storage capabilities, spanning on-premises and cloud? If so, all of the managed options have hybrid options.</p></li><li><p>Is your budget tight? If so, and if you’re OK with some manual planning and work to minimize the downtime that’s possible with any single point of failure, then a single Windows Compute Engine instance file server will do fine. </p></li><li><p>Do you require geo-diverse disaster recovery? You’re in luck—every option described here offers a path to DR.</p></li></ul><h3>What next?  </h3><p>This post serves as a brief overview of several options for Windows file services in Google Cloud. <a href="https://cloud.google.com/solutions/filers-on-compute-engine">Take a closer look</a> at the ones that interest you. Once you’ve narrowed it down to the top candidates, you can go through the Marketplace pages (for the managed services) to get more info or start the process of launching the service. The self-managed options above include links to Google Cloud-specific instructions to get you started, then general Microsoft documentation to deploy your chosen cluster option.</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/storage-data-transfer/introducing-filestore-backups/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Filestore Backups eases migration of file-based apps to cloud</h4><p class="uni-related-article-tout__body">The new Filestore Backups lets you migrate your copy data services and backup strategy for your file systems in Google Cloud.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/products/storage-data-transfer/move-windows-workloads-to-cloud-with-google-cloud-storage/
Mon, 07 Dec 2020 17:00:00 -0000;Keeping students, universities and employers connected with Cloud SQL;<div class="block-paragraph"><div class="rich-text"><p><i><b>Editor’s note</b>: Today we’re hearing from Handshake, an innovative startup and platform that partners with universities and employers to ensure that college students have equal access to meaningful career opportunities. With over 7 million active student users, 1,000 university and 500,000 employer partners, it’s now the leading early career community in the U.S. Here’s how they migrated to Google Cloud SQL.</i></p><p>At <a href="https://joinhandshake.com/" target="_blank">Handshake</a>, we serve students and employers across the country, so our technology infrastructure has to be reliable and flexible to make sure our users can access our platform when they need it. In 2020, we’ve expanded our online presence, adding virtual solutions and establishing new partnerships with community colleges and bootcamps to increase career opportunities for our student users.</p><p>These changes and our overall growth would have been harder to implement on Heroku, our previous cloud service platform. Our website application, running on Rails, uses a sizable cluster and PostgreSQL as our primary data store. As we grew, we were finding Heroku to be increasingly expensive at scale. </p><p>To reduce maintenance costs, boost reliability, and provide our teams with increased flexibility and resources, Handshake migrated to Google Cloud in 2018, choosing to have our data managed through <a href="https://cloud.google.com/sql">Google Cloud SQL</a>. </p><h3>Cloud SQL freed up time and resources for new solutions</h3><p>This migration proved to be the right decision. After a relatively smooth migration over a six-month period, our databases are completely off of Heroku now. Cloud SQL is now at the heart of our business. We rely on it for nearly every use case, continuing with a sizable cluster and using PostgreSQL as our sole owner of data and source of truth. Virtually all of our data, including information about our students, employers, and universities, is in PostgreSQL. Anything in our website is translated to a data model that’s reflected in our database.</p><p>Our main web application uses a monolithic database architecture. It uses an instance with one primary and one read replica and it has 60 CPUs, almost 400 GB of memory, and 2 TB of storage, of which 80 percent is utilized.</p></div></div><div class="block-pull_quote"><div class="uni-pull-quote h-c-page"><section class="h-c-grid"><div class="uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><div class="uni-pull-quote__inner-wrapper h-c-copy h-c-copy"><q class="uni-pull-quote__text">Cloud SQL is at the heart of our business, providing our startup with enterprise-level features.</q> <cite class="uni-pull-quote__author"><span class="uni-pull-quote__author-meta"><strong class="h-u-font-weight-medium">Rodney Perez, Infrastructure Engineer</strong><br /></span></cite></div></div></section></div></div><div class="block-paragraph"><div class="rich-text"><p>Several Handshake teams use the database, including Infrastructure, Data, Student, Education, and Employer teams. The data team is usually interacting with the transactional data, writing pipelines, pulling data out of PostgreSQL and loading it into <a href="https://cloud.google.com/bigquery">BigQuery</a> or Snowflake. We run a separate replica for all of our databases, specifically for the data team, so they can export without a performance hit. </p><p>With most managed services, there will always be maintenance that requires downtime, but with Cloud SQL, any necessary maintenance is easy to schedule. If the Data team needs more memory, capacity, or disk space, our Infrastructure team can coordinate and decide if we need a maintenance window or a similar approach that involves zero downtime. </p><p>We also use <a href="https://cloud.google.com/memorystore">Memorystore</a> as a cache and heavily leverage Elasticsearch. Our Elasticsearch index system uses a separate PostgreSQL instance for batch processing. Whenever there are record changes inside our main application, we send a <a href="https://cloud.google.com/pubsub">Pub/Sub</a> message from which the indexers queue off, and they’ll use that database to help with that processing, putting that information into Elasticsearch and creating those indices. </p><h3>Nimble, flexible and planning for the future</h3><p>With Cloud SQL managing our databases, we can devote resources toward creating new services and solutions. If we had to run our own PostgreSQL cluster, we’d need to hire a database administrator. Without Cloud SQL’s service-level agreement (SLA) promises, if we were setting up a PostgreSQL instance in a <a href="https://cloud.google.com/compute">Compute Engine virtual machine</a>, our team would have to double in size to handle the work that Google Cloud now manages. Cloud SQL also offers automatic provisioning and storage capacity management, saving us additional valuable time. </p><p>We’re generally far more read-heavy than write-heavy, and our future plans for our data with Cloud SQL include offloading more of our reads to read replicas, and keeping the primary for just writes, using PgBouncer in front of the database to decide where to send which query. </p><p>We are also exploring committed use discounts to cover a good baseline of our usage. We still want to have the flexibility to do cost cutting and reduce our usage where possible, and to realize some of those initial savings right away. Also, we’d like to split up the monolith into smaller databases to reduce the blast radius, so that they can be tuned more effectively to each use case. </p><p>With Cloud SQL and related services from Google Cloud freeing time and resources for Handshake, we can continue to adapt and meet the evolving needs of students, colleges, and employers.</p><p>Read more about <a href="https://joinhandshake.com/" target="_blank">Handshake</a> and the solutions we found in <a href="https://cloud.google.com/sql">Cloud SQL</a>.</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/databases/cloud-sql-database-service-adds-postgresql-13/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Cloud SQL now supports PostgreSQL 13</h4><p class="uni-related-article-tout__body">Fully managed Cloud SQL cloud database service now supports PostgreSQL 13.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/products/databases/running-google-cloud-sql-for-high-performance-lower-cost/
Mon, 07 Dec 2020 14:05:00 -0000;Google Cloud fuels new discoveries in astronomy;<div class="block-paragraph"><div class="rich-text"><p>From understanding our origins to predicting future events, some of the greatest breakthroughs we’ve made on Earth have come from studying the universe. <a href="https://cloud.google.com/solutions/hpc">High-performance computing</a> and machine learning (ML) are accelerating this kind of research at an unprecedented pace. At Google Cloud, we’re proud to play even a small role in advancing the science of astronomy—and that’s why we’re excited today to highlight new work with the <a href="https://www.lsst.org/" target="_blank">Vera C. Rubin Observatory</a> in Chile and researchers at the California Institute of Technology. </p><h3>The cloud foundation for 20TB of nightly sky observations</h3><p>In a pioneering collaboration, the Rubin Observatory has finalized a three-year agreement to host its Interim Data Facility (IDF) on Google Cloud. Through this collaboration, Rubin will process astronomical data collected by the observatory and make the data available to hundreds of users in the scientific community in advance of its 10-year <a href="https://www.lsst.org/about" target="_blank">Legacy Survey of Space and Time (LSST)</a> project. </p><p>The LSST aims to conduct a deep survey over an enormous area of sky to create an astronomical catalog thousands of times larger than any previously compiled survey. Using the 8.4-meter Simonyi Survey Telescope and the gigapixel LSST Camera, the survey will capture about 1,000 images of the sky every night for 10 years. These high-resolution images will contain data for roughly 20 billion galaxies and a similar number of stars, providing researchers with an unparalleled resource for understanding the structure and evolution of our universe over time. By building the IDF on Google Cloud, Rubin Observatory will lay the foundation to manage a massive dataset—500 petabytes in total—that will eventually be shared with the scientific community at scale, and with flexibility. </p><p>“We’re extremely pleased to work with Google Cloud on this project, which will have a big and positive impact on our ability to deliver for the Rubin community,” says Bob Blum, acting director of operations for Rubin Observatory.</p><p>“We don’t have to build the infrastructure ourselves—it’s well-established and has been tested and improved for other users, so we benefit from that,” explains Hsin-Fang Chiang, data management science analyst and engineer for Rubin Observatory, and one of the early users of the IDF. </p><p>The Rubin Observatory will use <a href="https://cloud.google.com/storage">Google Cloud Storage</a> and <a href="https://cloud.google.com/kubernetes-engine">Google Kubernetes Engine</a>, and Google Workspace will enable productivity and collaboration.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Rubin Observatory.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Rubin_Observatory.max-1000x1000.jpg" /><figcaption class="article-image__caption "><div class="rich-text"><i>Rubin Observatory at sunset, lit by a full moon</i></div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><h3>Caltech researcher discovers new comet with AI</h3><p>While comet sightings are relatively common, the discovery of a new comet is rare. The <a href="https://minorplanetcenter.net/" target="_blank">Minor Planet Center</a>, which tracks the solar system’s minor bodies in space, cataloged fewer than 100 new comets in 2019, as opposed to about 21,000 new minor planets. </p><p>In late August 2020, Dr. Dmitry Duev, research scientist in the Astronomy department at Caltech, began a pilot program to use Google Cloud’s tools to identify the objects observed by the <a href="https://www.ztf.caltech.edu/" target="_blank">Zwicky Transient Facility</a> (ZTF) at the Palomar Observatory in Southern California. The ZTF scans the Northern skies every clear night, measuring billions of astronomical objects and registering millions of transient events. Using these images, Duev trained an ML model on Google Cloud to pinpoint comets with over 99% accuracy. On October 7, the model identified Comet C/2020 T2, the first ever such discovery attributed to artificial intelligence. This achievement makes the discovery of new comets possible at a greatly accelerated rate. </p><p>“Having a fast and accurate way to classify objects we see in the sky is revolutionizing our field,” Duev says. “It’s like having a myriad of highly trained astronomers at our disposal 24/7.”</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="orbit of comet C_2020 T2.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/orbit_of_comet_C_2020_T2.max-1000x1000.jpg" /><figcaption class="article-image__caption "><div class="rich-text"><i>The orbit of comet C/2020 T2 as of October 7, 2020.</i><p><i>Image credit: NASA/JPL-Caltech / D. Duev.</i></p></div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><h3>Interested in using Google Cloud to unlock the secrets of the universe?</h3><p>These are just a few of the fascinating projects we’re working on with our customers in astronomy. In April 2019, the <a href="https://eventhorizontelescope.org/" target="_blank">Event Horizon Telescope</a>, a virtual combination of eight radio telescopes from all over the world, used Google Cloud’s <a href="https://cloud.google.com/compute/docs/instances">virtual machine</a> (VMs) i<a href="https://cloud.google.com/compute/docs/instances">nstances</a> to produce the<a href="https://edu.google.com/why-google/case-studies/eht-gcp/" target="_blank">first image of a supermassive black hole</a>. And, since 2018, Google has also been working in partnership with the <a href="https://frontierdevelopmentlab.org/" target="_blank">Frontier Development Lab</a> on applying machine learning to some of NASA’s most challenging problems in our universe: <a href="https://www.youtube.com/watch?v=w36rwUUtaVI&amp feature=youtu.be" target="_blank">forecasting floods</a> here on Earth, <a href="https://www.youtube.com/watch?v=dcgJXmq7Ep4&amp feature=youtu.be" target="_blank">finding minerals</a> on the moon to support a permanent base there, and <a href="https://www.youtube.com/watch?v=CMDRHTiJKUA&amp feature=youtu.be" target="_blank">predicting solar flares</a> that can interrupt satellite communications.</p><p>To start or ramp up your own project, we offer research credits to academics using Google Cloud for qualifying projects in eligible countries. You can find <a href="https://lp.google-mkto.com/gcp-research-credits.html" target="_blank">our application form</a> on Google Cloud’s <a href="http://cloud.google.com/solutions/education">website</a> or contact our sales team. To learn more about powering research in astronomy and other fields, <a href="https://cloudonair.withgoogle.com/events/public-sector-summit?utm_source=google&amp utm_medium=blog&amp utm_campaign=-&amp astronomy-blog" target="_blank">register</a> for the Google Cloud Public Sector Summit which features many research sessions. The sessions launch December 8-9 and will also be available on demand.</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/ai-machine-learning/is-there-life-on-other-planets-google-cloud-is-working-with-nasas-frontier-development-lab-to-find-out/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Is there life on other planets? Google Cloud is working with NASA's Frontier Development Lab to find out</h4><p class="uni-related-article-tout__body">Google Cloud collaborates with NASA's Frontier Development Lab to pursue astrobiological analytics, in an attempt to profile the atmosphe...</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/topics/public-sector/announcing-new-work-with-the-rubin-observatory/
Fri, 04 Dec 2020 17:00:00 -0000;Machine learning patterns with Apache Beam and the Dataflow Runner, part I;<div class="block-paragraph"><div class="rich-text"><p>Over the  years, businesses have increasingly used Dataflow for its ability to pre-process stream and/or batch data for machine learning. Some success stories include <a href="https://cloud.google.com/customers/harambee/">Harambee</a>, <a href="https://cloud.google.com/customers/monzo/">Monzo</a>, <a href="https://cloud.google.com/customers/dow-jones/">Dow Jones</a>, and <a href="https://cloud.google.com/customers/fluidly/">Fluidly</a>.</p><p>A growing number of other customers are using machine learning inference in Dataflow pipelines to extract insights from data. Customers have the choice of either using ML models loaded into the Dataflow pipeline itself, or calling ML APIs provided by Google Cloud. </p><p>As these use cases develop, there are some common patterns being established which will be explored in this series of blog posts. In part I of this series, we’ll explore the process of providing a model with data and extracting the resulting output, specifically:</p><ul><li><p>Local/remote inference efficiency patterns</p></li><ul><li><p>Batching pattern</p></li><li><p>Singleton model pattern</p></li></ul><li><p>Multi-model inference pipelines</p></li><ul><li><p>Data branching to get the data to multiple models</p></li><li><p>Joining results from multiple branches</p></li></ul></ul><p>Although the programming language used throughout this blog is Python, many of the general design patterns will be relevant for other languages supported by Apache Beam pipelines. This also holds true for the ML framework here we are using TensorFlow but many of the patterns will be useful for other frameworks like PyTorch and XGBoost. At its core, this is about delivering data to a model transform and the post processing of that data downstream.</p><p>To make the patterns more concrete for the local model use case, we will make use of the open source "Text-to-Text Transfer Transformer” (T5) model which was published in “<a href="https://arxiv.org/abs/1910.10683" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>”. The paper presents a large-scale empirical survey to determine which transfer learning techniques for language modelling work best, and applies these insights at scale to produce a model that achieves state-of-the-art results on numerous NLP tasks.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 1.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_1.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>In the sample code, we made use of the "Closed-Book Question Answering" ability, as explained in the <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" target="_blank">T5 blog</a> </p><p><i>"...In our <a href="https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb" target="_blank">Colab</a> demo and follow-up <a href="https://arxiv.org/abs/2002.08910" target="_blank">paper</a>, we trained T5 to answer trivia questions in a more difficult ‘closed-book’ setting, without access to any external knowledge. In other words, in order to answer a question T5 can only use knowledge stored in its parameters that it picked up during unsupervised pre-training. This can be considered a constrained form of <a href="https://en.wikipedia.org/wiki/Question_answering#Open_domain_question_answering" target="_blank">open-domain question answering</a>." </i></p><p>For example, we ask the question, "How many teeth does a human have?" and the model returns with “20 primary.” The model is well suited for our discussion, as in its largest incarnation it has over 11 billion parameters and is over 25 Gigabytes in size, which necessitates following the good practices described in this blog. </p><h2>Setting up the T5 Model</h2><p>There are several sizes of the T5 model, in this blog we will make use of  small and XXL sizes. Given the very large memory footprint needed by the XXL mode (25 GB for the save model files), we recommend working with the small version of the model when exploring most of the code samples below. You can download instructions from the T5 team in this <a href="https://colab.sandbox.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-deploy.ipynb#scrollTo=UHCx-R4M-D0a" target="_blank">colab</a>. </p><p>For the final code sample in this blog, you’ll need the XXL model, we recommend running that code via python command on a machine with 50+ GB of memory.</p><p>The default for the T5 model export is to have an inference batch size of 1. For our purposes, we’ll need this to be set to 10 by adding --batch_size=10 as seen in the code sample below.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><h3>Batching pattern</h3><p>A pipeline can access a model either locally (internal to the pipeline) or remotely (external to the pipeline).  </p><p>In Apache Beam, a data processing task is described by a pipeline, which represents a directed acyclic graph (DAG) of transformations (<a href="https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.transforms.ptransform.html#apache_beam.transforms.ptransform.PTransform" target="_blank">PTransforms</a>) that operate on collections of data (PCollections). A pipeline can have multiple PTransforms, which can execute user code defined in do-functions (DoFn, pronounced as do-fun) on elements of a PCollection. This work will be distributed across workers by the Dataflow runner, scaling out resources as needed.</p><p>Inference calls are made within the DoFn. This can be through the use of functions that load models locally or via a remote call, for example via HTTP, to an external API endpoint. Both of these options require specific considerations in their deployment, and these patterns are explored below.</p><p><b>Inference flow<br /></b>Before we outline the pattern, let's look at the various stages of making a call to an inference function within our DoFn.</p><ol><li><p>Convert the raw data to the correct serialized format for the function we are calling. </p></li><li><p>Carry out any preprocessing required.</p></li><li><p>Call the inference function:</p></li><ol><li><p>In local mode: </p></li><ol><li><p>Carry out any initialization steps needed (for example loading the model). </p></li><li><p>Call the inference code with the serialized data.</p></li></ol><li><p>In remote mode, the serialized data is sent to an API endpoint, which requires establishing a connection, carrying out authorization flows, and finally sending the data payload.</p></li></ol><li><p>Once the model processes the raw data, the function returns with the serialized result.</p></li><li><p>Our DoFn can now deserialize the result ready for postprocessing.</p></li></ol><p>The administration overhead of initializing the model in the local case, and the connection/auth establishment in the remote case, can become significant parts of the overall processing. It is possible to reduce this overhead by batching before calling the inference function. Batching allows us to amortize the admin costs across many elements, improving efficiency. </p><p>Below, we discuss several ways you can achieve batching with Apache Beam, as well as ready made implementations of these methods.</p><p><b>Batching through Start/Finish bundle lifecycle events<br /></b>When an Apache Beam runner <a href="https://beam.apache.org/documentation/runtime/model/" target="_blank">executes pipelines</a>, every DoFn instance processes zero or more “bundles'' of elements. We can use <a href="https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.transforms.core.html?highlight=start_bundle#apache_beam.transforms.core.DoFn" target="_blank">DoFn's life cycle events</a> to initialize resources shared between bundles of work. The helper transform <a href="https://beam.apache.org/releases/pydoc/2.24.0/apache_beam.transforms.util.html#apache_beam.transforms.util.BatchElements" target="_blank"><code>BatchElements</code></a> leverages start_bundle and finish_bundle methods to regroup elements into batches of data, optimizing the batch size for amortized processing.  </p><p><i>Pros</i>:  No shuffle step is required by the runner. <br /><i>Cons</i>: Bundle size is determined by the runner. In batch mode, bundles are large, but in stream mode bundles can be very small.</p><hr /><p></p><p>Note: BatchElements attempts to find optimal batch sizes based on runtime performance. <br /><br /><i>"This transform attempts to find the best batch size between the minimum and maximum parameters by profiling the time taken by (fused) downstream operations. For a fixed batch size, set the min and max to be equal."</i> (<a href="https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.transforms.util.html" target="_blank">Apache Beam documentation</a>) </p><p>In the sample code we have elected to set both min and max for consistency.</p><hr /><p>In the example below, sample questions are created in a batch ready to send to the T5 model:<br /></p><p></p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p><b>Batching through state and timers <br /></b>The <a href="https://beam.apache.org/documentation/programming-guide/#types-of-state" target="_blank">state and timer API</a> is the primitives within Apache Beam which other higher level primitives like windows are built on. Some of the public batching mechanisms used for making calls to Google Cloud APIs like the <a href="https://cloud.google.com/dlp">Cloud Data Loss Prevention API</a> via <a href="https://github.com/GoogleCloudPlatform/dlp-dataflow-deidentification" target="_blank">Dataflow templates</a>, rely on this mechanism. The helper transform <a href="https://beam.apache.org/releases/pydoc/2.24.0/apache_beam.transforms.util.html#apache_beam.transforms.util.GroupIntoBatches" target="_blank">GroupIntoBatches</a> leverages the state and timer API to group elements into batches of desired size. Additionally it is key-aware and will batch elements within a key. </p><p><i>Pros</i>: Fine-grained control of the batch, including the ability to make data driven decisions.<br /><i>Cons</i>: Requires shuffle.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p><b>Combiners<br /></b>Apache Beam <a href="https://beam.apache.org/documentation/programming-guide/#combine" target="_blank">Combiner API</a> allows elements to be combined within a PCollection, with variants that work on the whole PCollection or on a per key basis. As Combine is a common transform, there are a lot of examples of its usage in the core documents.</p><p><i>Pros</i>: Simple API<br /><i>Cons</i>: Requires shuffle. Coarse-grained control of the output.</p><p>With these techniques, we will now have a batch of data to use with the model, including the initialization cost, now amortized across the batch. There is more that can be done to make this work efficient, particularly for large models when dealing with local inference. In the next section we will explore inference patterns. </p><h2>Remote/local inference</h2><p>Now that we have a batch of data that we would like to send to a model for inference, the next step will depend on whether the inference will be local or remote. </p><p><b>Remote inference<br /></b>In remote inference, a remote procedure call is made to a service outside of the Dataflow pipeline. For a custom built model, the model could be hosted, for example on a Kubernetes cluster or through a managed service such as <a href="https://cloud.google.com/ai-platform/prediction/docs">Google Cloud AI Platform Prediction</a>. For pre-built models which are provided as a service, the call will be to the service endpoint, for example <a href="https://cloud.google.com/document-ai">Google Cloud Document AI</a>. The major advantage of using remote inference is that we do not need to assign pipeline resources to loading the model, or take care of versions.</p><p>Factors to consider with remote calls:</p><ul><li><p>Ensure that the total batch size is within the limits provided by the service. </p></li><li><p>Ensure that the endpoint being called is not overwhelmed, as Dataflow will spin up resources to deal with the incoming load. You can limit the total number of threads being used in the calls by several options:</p></li><ul><li><p>Set the max_num_workers value within the pipeline options.</p></li><li><p>If required, make use of worker process/thread control (discussed in more depth later in this blog).</p></li></ul></ul><p>In circumstances when remote inference is not possible, the pipeline will also need to deal with actions like loading the model and sharing that model across multiple threads. We’ll look at these patterns next.  </p><p><b>Local inference<br /></b>Local inference is carried out by loading the model into memory. This heavy initialization action, especially for larger models, can require more than just the Batching pattern to perform efficiently. As discussed before, the user code encapsulated in the DoFn is called against every input. It would be very inefficient, even with batching, to load the model on every invocation of the DoFn.process method.</p><p>In the ideal scenario the model lifecycle will follow this pattern:</p><ol><li><p>Model is loaded into memory by the transformation used for prediction work.</p></li><li><p>Once loaded, the model serves data, until an external life cycle event forces a reload.</p></li></ol></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 2.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_2.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Part of the way we reach this pattern is to make use of the shared model pattern, described in detail below.</p><h3>Singleton model (shared.py)</h3><p>The shared model pattern allows all threads from a worker process to make use of a single model by having only one instance of the model loaded into memory per process.  This pattern is common enough that the <a href="https://beam.apache.org/releases/pydoc/2.24.0/apache_beam.utils.shared.html" target="_blank">shared.py</a> utility class has been made available in Apache Beam since version 2.24.0. </p><p><b>End-to-end local inference example with T5 model<br /></b>In the below code example, we will apply both the batching pattern as well as the shared model pattern to create a pipeline that makes use of the T5 model to answer general knowledge questions for us.</p><p>In the case of the T5 model, the batch size we specified requires the array of data that we send to it to be exactly of length 10. For the batching, we will make use of the <a href="https://beam.apache.org/releases/pydoc/2.24.0/apache_beam.transforms.util.html#apache_beam.transforms.util.BatchElements" target="_blank">BatchElements</a> utility class. An important consideration with BatchElements is that the batch size is a target, not a guarantee of size. For example, if we have 15 examples, then we might get two batches one of 10 and one of 5. This is dealt with in the processing functions shown in the code.</p><p>Please note the inference call is done directly via <code>model.signatures</code> as a simple way to show the application of the shared.py pattern, which is to load a large object once and then reuse.  (The code lab <a href="https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb" target="_blank">t5-trivia</a> shows an example of wrapping the predict function).</p><hr /><p>Note: Determining the optimum batch size is very workload specific and would warrant an entire blog discussion on its own. Experimentation as always the key for understanding the optimum size/latency.<br /></p><hr /><p>Note: If the object you are using for shared.py can not be safely called from multiple threads, you can make use of a locking mechanism. This will limit parallelism on the worker, but the trade off may still be useful depending on the size / initialization cost of loading the model. </p><hr /></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Running the code sample will produce the following output (when using the small T5 model):</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><h2>Worker thread/process control (advanced) </h2><p>With most models, the techniques we have described so far will be enough to run an efficient pipeline. However, in the case of extremely large models like the T5 XXL, you will need to provide more hints to the runner to ensure that the workers have enough resources to load the model. We are working on improving this and we will remove the needs for these parameters eventually. But until then, use this if your models need it.</p><p>A single runner is capable of running many processes and threads on a worker, as shown in the diagram below:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 3.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_3.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>The parameters detailed below are those that can be used with the Dataflow Runner v2. Runner v2 is currently available using the flag --experiments=use_runner_v2.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 4.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_4_I13Evve.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>To ensure that the total_memory/num processes are at a ratio that can support large models, these values will need to be set as follows:</p><ul><li><p>If using the shared.py pattern, the model will be shared across all threads but not across processes. </p></li><li><p>If not using the shared.py pattern and the model is loaded, for example, within the @setup DoFn lifecycle event, then make use of number_of_worker_harness_threads to match the memory of the worker.</p></li></ul><h3>Multiple-model inference pipelines</h3><p>In the previous set of patterns, we covered the mechanics of enabling efficient inference. In this section, we will look at some functional patterns which allow us to leverage the ability to create multiple inference flows within a single pipeline. </p><h2>Pipeline branches</h2><p>A branch allows us to flow the data in a PCollection to different transforms. This allows multiple models to be supported in a single pipeline, enabling useful tasks like: </p><ul><li><p>A/B testing using different versions of a model. </p></li><li><p>Having different models produce output from the same raw data, with the outputs fed to a final model.</p></li><li><p>Allowing a single data source to be enriched and shaped in different ways for different use cases with separate models, without the need for multiple pipelines.</p></li></ul><p>In Apache Beam, there are two easy options to create a branch in the inference pipeline. One is by applying multiple transformations to a PCollection:</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>The other uses multi-output transforms:</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><h2>Using T5 and the branch pattern </h2><p>As we have multiple versions of our T5 model (small and XXL), we can run some tests which branch the data, carry out inference on different models, and join the data back together to compare the results. </p><p>For this experiment, we will use a more ambiguous question of the form. </p><p>"Where does the name {first name} come from." </p><p>The intent of the question is to determine the origins of the first names. Our assumption is that the XXL model will do better with these names than the small model. </p><p>Before we build out the example, we first need to show how to enhance the previous code to give us a way to bring the results of two separate branches back together. Using the previous code, the predict function can be changed by merging the questions with the inferences via zip().</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p><b>Building the pipeline<br /></b>The pipeline flow is as follows:</p><ol><li><p>Read in the example questions.</p></li><li><p>Send the questions to the small and XXL versions of the model via different branches.</p></li><li><p>Join the results back together using the question as the key.</p></li><li><p>Provide simple output for visual comparison of the values.</p></li></ol><hr /><p>Note: To run this code sample with the  XXL model and the directrunner, you will need a machine with a minimum of 60GB of memory. You can also of course run this example code with any of the other sizes that fall between the small to XXL which will have a lower memory requirement.<br /></p><hr /></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>The output is shown below.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>As we can see, the larger XXL model did a lot better than the small version of the model. This makes sense as the additional parameters allow the model to store more world knowledge. This result is confirmed by findings of <a href="https://arxiv.org/abs/2002.08910" target="_blank">https://arxiv.org/abs/2002.08910</a>". </p><p>Importantly, we now have a tuple which contains the predictions from both of the models which can be easily used downstream. </p><p>Below we can see the shape of the graph produced by the above code when run on the Dataflow Runner.</p><hr /><p></p><p>Note: To run the sample on the Dataflow runner, please make use of a <a href="https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/" target="_blank">setup.py</a> file with the install_requires parameters as below, the tensorflow-text is important as the T5 model requires the library even though it is not used directly in the code samples above.</p><p>install_requires=['t5==0.7.1', 'tensorflow-text==2.3.0', 'tensorflow==2.3.1']<br />A high memory machine will be needed with the XXL model, the pipeline above was run with configuration:<br />machine_type = custom-1-106496-ext<br />number_of_worker_harness_threads = 1<br />experiment = use_runner_v2</p><p>As the XXL model is &gt 25 Gig in size, with the load operation taking more than 15 mins. To reduce this load time, use a <a href="https://beam.apache.org/documentation/runtime/environments/" target="_blank">custom container</a>.<br />The predictions with the XXL model can take many minutes to complete on a CPU.</p><hr /><p></p></div></div><div class="block-paragraph"><div class="rich-text"><p>Batching and branching:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 5.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_5.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Joining the results:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 6.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_6.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><h3>Conclusion</h3><p>In this blog, we covered some of the patterns for running remote/local inference calls, including batching, the singleton model pattern, and understanding the processing/thread model for dealing with large models. Finally, we touched on how the easy creation of complex pipeline shapes can be used for more advanced inference pipelines. </p><p>To learn more, review the <a href="https://cloud.google.com/dataflow/docs">Dataflow documentation</a>.</p></div></div>;https://cloud.google.com/blog/products/data-analytics/ml-inference-in-dataflow-pipelines/
Fri, 04 Dec 2020 17:00:00 -0000;Get to know Workflows, Google Cloud’s serverless orchestration engine;<div class="block-paragraph"><div class="rich-text"><p>Whether your company is processing e-commerce transactions, producing goods or delivering IT services, you need to manage the flow of work across a variety of systems. And while it’s possible to manage those workflows manually or with general-purpose tools, doing so is much easier with a purpose-built product. </p><p>Google Cloud has two workflow tools in its portfolio: <a href="https://cloud.google.com/composer">Cloud Composer</a> and the new <a href="https://cloud.google.com/workflows">Workflows</a>. <a href="https://cloud.google.com/blog/topics/google-cloud-next/developer-productivity-announcements-at-next20-onair">Introduced in August</a>, Workflows is a fully managed workflow orchestration product running as part of Google Cloud. It’s fully serverless and requires no infrastructure management.</p><p>In this article we’ll discuss some of the use cases that Workflows enables, its features, and tips on using it effectively.</p><h3>A sample workflow</h3><p>First, consider the following workflow for generating an invoice:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 1.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_1_sdF2Tdx.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>A common way to orchestrate these steps is to call API services based on <a href="https://cloud.google.com/functions/docs">Cloud Functions</a>, <a href="https://cloud.google.com/run">Cloud Run</a> or a public SaaS API, e.g. SendGrid, which sends an e-mail with our PDF attachment. But real-life scenarios are typically much more complex than the example above and require continuous tracking of all workflow executions, error handling, decision points and conditional jumps, iterating arrays of entries, data conversions and many other advanced features. </p><p>Which is to say, while technically you can use general-purpose tools to manage this process, it’s not ideal. For example, let’s consider some of the challenges you’d face processing this flow with an event-based compute platform like Cloud Functions. First, the max duration of a Cloud Function run is nine minutes, but workflows—especially those involving human interactions—can run for days your workflow may need more time to complete, or you may need to pause in between steps when polling for a response status. Attempting to chain multiple Cloud Functions together with for instance, <a href="https://cloud.google.com/pubsub">Pub/Sub</a> also works, but there’s no simple way to develop or operate such a workflow. First, in this model it’s very hard to associate step failures with workflow executions, making troubleshooting very difficult. Also, understanding the state of all workflow executions requires a custom-built tracking model, further increasing the complexity of this architecture. </p><p>In contrast, workflow products provide support for exception handling and give visibility on executions and the state of individual steps, including successes and failures. Because the state of each step is individually managed, the workflow engine can seamlessly recover from errors, significantly improving reliability of the applications that use the workflows. Lastly, workflow products often come with built-in connectors to popular APIs and cloud products, saving time and letting you plug into existing API interfaces. </p><h3>Workflow products on Google Cloud</h3><p>Google Cloud’s first general purpose workflow orchestration tool was Cloud Composer.</p><p>Based on Apache Airflow, Cloud Composer is great for data engineering pipelines like ETL orchestration, big data processing or machine learning workflows, and integrates well with data products like BigQuery or Dataflow . For example, Cloud Composer is a natural choice if your workflow needs to run a series of jobs in a data warehouse or big data cluster, and save results to a storage bucket.</p><p>However, if you want to process events or chain APIs in a serverless way—or have workloads that are bursty or latency-sensitive—we recommend Workflows. </p><p>Workflows scales to zero when you’re not using it, incurring no costs when it’s idle. Pricing is based on the number of steps in the workflow, so you only pay if your workflow runs. And because Workflows doesn’t charge based on execution time, if a workflow pauses for a few hours in between tasks, you don’t pay for this either. </p><p>Workflows scale up automatically with very low startup time and no “cold start” effect. Also, it transitions quickly between steps, supporting latency-sensitive applications. </p><h3>Workflows use cases</h3><p>When it comes to the number of processes and flows that Workflows can orchestrate, the sky’s the limit. Let’s take a look at some of the more popular use cases. </p><p><b>Processing customer transactions</b></p><p>Imagine you need to process customer orders and, in the case that an item is out of stock, trigger an inventory refill from an external supplier. During order processing you also want to notify your sales reps about large customer orders. Sales reps are more likely to react quickly if they get such notifications using Slack. </p><p>Here is an example workflow diagram.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 2.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_2_LBi7g2Y.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>The workflow above orchestrates calls to Google Cloud’s Firestore as well as external APIs including Slack, SendGrid or the inventory supplier’s custom API. It passes the data between the steps and implements decision points that execute steps conditionally, depending on other APIs’ outputs. </p><p>Each workflow execution—handling one transaction at a time—is logged so you can trace it back or troubleshoot it if needed. The workflow handles necessary retries or exceptions thrown by APIs, thus improving the reliability of the entire application. </p><p><b>Processing uploaded files</b></p><p>Another case you may consider is a workflow that tags files that users have uploaded based on file contents. Because users can upload text files, images or videos, the workflow needs to use different APIs to analyze the content of these files. </p><p>In this scenario, a Cloud function is triggered by a Cloud Storage trigger. Then, the function starts a workflow using the Workflows client library, and passes the file path to the workflow as an argument. </p><p>In this example, a workflow decides which API to use depending on the file extension, and saves a corresponding tag to a Firestore database.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 3.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_3_4zhI67V.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><h3>Workflows under the hood</h3><p>You can implement all of these use cases out of the box with Workflows. Let’s take a deeper look at some key features you’ll find in Workflows. </p><p><b>Steps</b></p><p>Workflows handles sequencing of activities delivered as ‘steps’. If needed, a workflow can also be configured to pause between steps without generating time-related charges.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 4.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_4_VMSRx8k.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>In particular, you can orchestrate practically any API that is network-reachable and follows HTTP as a workflow step. You can make a call to any internet-based API, including SaaS APIs or your private endpoints, without having to wrap such calls in Cloud Functions or Cloud Run.</p><p><b>Authentication</b></p><p>When making calls to Google Cloud APIs, e.g., to invoke a Cloud function or read data from Firestore, Workflows uses built-in IAM authentication. As long as your workflow has been granted IAM permission to use a particular Google Cloud API, you don’t need to worry about authentication protocols.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 5.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_5_a0rN0DD.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p><b>Communication between workflow steps</b></p><p>Most real-life workflows require that steps communicate with one another. Workflows supports built-in variables that steps can use to pass the result of their work to a subsequent step. </p><p><b>Automatic JSON conversion</b></p><p>As JSON is very common in API integrations, Workflows automatically converts API JSON responses to dictionaries, making it easy for the following steps to access this information. </p><p><b>Rich expression language</b></p><p>Workflows also comes with a rich expression language supporting arithmetic and logical operators, arrays, dictionaries and many other features. The ability to perform basic data manipulations directly in the workflow further simplifies API integrations. Because Workflows accepts runtime arguments, you can use a single workflow to react to different events or input data.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 6.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_6_1OUZuI5.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p><b>Decision points</b></p><p>With variables and expressions, we can implement another critical component of most workflows: decision points. Workflows can use custom expressions to decide whether to jump to another part of the workflow or conditionally execute a step. </p><p><b>Conditional step execution</b></p><p>Frequently used parts of the logic can be coded as a sub-workflow and then called as a regular step, working similarly to routines in many programming languages.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 7.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_7.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Sometimes, a step in a workflow fails, e.g., due to a network issue or because a particular API is down. This, however, shouldn’t immediately make the entire workflow execution fail. </p><p>Workflows avoids that problem with a combination of configurable retries and exception handling that together allow a workflow to react appropriately to an error returned by the API call.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="image 8.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image_8.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>All features above are configurable as part of the Workflows source code. You can see practical examples of these configurations<a href="https://cloud.google.com/workflows/docs/sample-workflows">here</a>. </p><h3>Get started with Workflows today</h3><p>Workflows is a powerful new addition to Google Cloud’s application development and management toolset, and you can try it out immediately on all your projects. </p><p>Have a look at the<a href="http://cloud.google.com/workflows">Workflows site</a> or go right ahead to the<a href="http://console.cloud.google.com/workflows">Cloud Console</a> to build your first workflow. Workflows comes with a free tier so you can give it a try at no cost. Also, watch out for exciting Workflows announcements coming soon!</p><p>Happy orchestrating! :)</p></div></div>;https://cloud.google.com/blog/products/application-development/get-to-know-google-cloud-workflows/
Fri, 04 Dec 2020 17:00:00 -0000;Getting higher MPI performance for HPC applications on Google Cloud;<div class="block-paragraph"><div class="rich-text"><p>Most High Performance Computing (HPC) applications such as large-scale engineering simulations, molecular dynamics, and genomics, run on supercomputers or HPC clusters on-premises. Cloud is emerging as a great option for these workloads due to its elasticity, pay per use, and the lower associated maintenance cost.</p><p>Reducing Message Passing Interface (MPI) latency is one critical element of delivering HPC application performance and scalability. We recently introduced several features and tunings that make it easy to run MPI workloads and achieve optimal performance on Google Cloud. These best practices reduce MPI latency, especially for applications that depend on small messages and collective operations. </p><p>These best practices help optimize Google Cloud systems and networking infrastructure to improve MPI communication over TCP without requiring major software changes or new hardware support. With these best practices, MPI ping-pong latency falls into single-digits of microseconds (μs), and small MPI messages are delivered in 10μs or less. In the figure below, we show how progressive optimizations lowered one-way latency from 28 to 8μs with a test setup on Google Cloud.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Reducing MPI.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/reducing_mpi.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Improved MPI performance translates directly to improved application scaling, expanding the set of workloads that run efficiently on Google Cloud. If you plan to run MPI workloads on Google Cloud, use these practices to get the best possible performance. Soon, you will be able to use the upcoming HPC VM Image to easily apply these best practices and get the best out-of-the-box performance for your MPI workloads on Google Cloud.</p><h3>1. Use Compute-optimized VMs</h3><p><a href="https://cloud.google.com/blog/products/compute/introducing-compute-and-memory-optimized-vms-for-google-compute-engine">Compute-optimized</a> (C2) instances have a fixed virtual-to-physical core mapping and expose NUMA architecture to the guest OS. These features are critical for performance of MPI workloads. They also leverage second Generation Intel Xeon Scalable Processors (Cascade Lake), which can provide up to a 40% improvement in performance compared to previous generation instance types due to their support for a higher clock speed of 3.8 GHz, and higher memory bandwidth. </p><p>C2 VMs also support vector instructions (AVX2, AVX512). We have noticed significant performance improvement for many HPC applications when they are compiled with AVX instructions. </p><h3>2. Use compact placement policy </h3><p>A <a href="https://cloud.google.com/sdk/gcloud/reference/beta/compute/resource-policies/create/group-placement">placement policy</a> gives you more control over the placement of your virtual machines within a data center. A compact placement policy ensures instances are hosted in nodes nearby on the network, providing lower latency topologies for virtual machines within a single availability zone. Placement policy APIs currently allow creation of up to 22 C2 VMs.</p><h3>3. Use Intel MPI and collective communication tunings</h3><p>For the best MPI application performance on Google Cloud, we recommend the use of Intel MPI 2018. The choice of MPI collective algorithms can have a significant impact on MPI application performance and Intel MPI allows you to manually specify the algorithms and configuration parameters for collective communication. </p><p>This tuning is done using <a href="https://software.intel.com/content/www/us/en/develop/documentation/mpi-developer-reference-linux/top/command-reference/mpitune.html" target="_blank">mpitune</a> and needs to be done for each combination of the number of VMs and the number of processes per VM on C2-Standard-60 VMs with compact placement policies. Since this takes a considerable amount of time, we provide the <a href="https://github.com/GoogleCloudPlatform/hpc-tools" target="_blank">recommended Intel MPI collective algorithms</a> to use in the most common MPI job configurations.</p><p>For better performance of scientific computations, we also recommend use of Intel Math Kernel Library (<a href="https://software.intel.com/en-us/mkl" target="_blank">MKL</a>).</p><h3>4. Adjust Linux TCP settings</h3><p>MPI networking performance is critical for tightly coupled applications in which MPI processes on different nodes communicate frequently or with large data volume. You can tune these network settings for optimal MPI performance.</p><ul><li><p>Increase tcp_mem settings for better network performance</p></li><li><p>Use network-latency profile on CentOS to enable busy polling</p></li></ul><h3>5. System optimizations</h3><p><b>Disable Hyper-Threading<br /></b>For compute-bound jobs in which both virtual cores are compute bound, Intel Hyper-Threading can hinder overall application performance and can add nondeterministic variance to jobs. Turning off Hyper-Threading allows more predictable performance and can decrease job times. </p><p><b>Review security settings<br /></b>You can further improve MPI performance by disabling some built-in Linux security features. If you are confident that your systems are well protected, you can evaluate disabling the following security features as described in Security settings section of the <a href="https://cloud.google.com/solutions/best-practices-for-using-mpi-on-compute-engine#disable_linux_firewalls">best practices guide</a>:</p><ul><li><p>Disable Linux firewalls</p></li><li><p>Disable SELinux</p></li><li><p>Turn off Spectre and Meltdown Mitigation</p></li></ul><h2>Now let’s measure the impact  </h2><p>In this section we demonstrate the impact of applying these best practices through application-level benchmarks by comparing the runtime with select customers’ on-prem setups: </p><h3>(i) National Oceanic and Atmospheric Administration (NOAA) FV3GFS benchmarks</h3><p>We measured the impact of the best practices by running the NOAA FV3GFS benchmarks with the C768 model and 104 C2-Standard-60 Instances (3,120 physical cores). The expected runtime target, based on on-premise supercomputers, was 600 seconds. Applying these best practices provided a 57% improvement compared to baseline measurements—we were able to run the benchmark in 569 seconds on Google Cloud (faster than the on-prem supercomputer).</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="NOAA FV3GFS.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/noaa_fv3gfs.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><h3>(ii) ANSYS LS-DYNA engineering simulation software</h3><p>We ran the LS-DYNA 3 cars benchmark using C2-Standard-60 instances, AVX512 instructions and a compact placement policy. We measured scaling from 30 to 120 MPI ranks (1-4 VMs) . By implementing these best practices, we achieved on-par or better runtime performance on Google Cloud in many cases when compared with the customer’s on-prem setup with specialized hardware.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="ANSYS LS-DYNA.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/ansys_ls-dyna.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><h2>There is more: easy and efficient application of best practices </h2><p>To simplify deployment of these best practices, we created an HPC VM Image based on CentOS 7 and that makes it easy to apply these best practices and get the best out-of-the-box performance for your MPI workloads on Google Cloud. You can also apply the tunings to your own image, using the bash and Ansible scripts published in the <a href="https://github.com/GoogleCloudPlatform/hpc-tools" target="_blank">Google HPC-Tools Github</a> repository or by following the <a href="https://cloud.google.com/solutions/best-practices-for-using-mpi-on-compute-engine">best practice guide</a>.</p><p>To request access to HPC VM Image, please sign up via this <a href="https://forms.gle/5T2agg8sbXxYiyxG6" target="_blank">form.</a> We recommend benchmarking your applications to find the most efficient or cost-effective configuration.</p><p>Applying these best practices can improve application performance and reduce cost. To further reduce and manage costs, we also offer automatic <a href="https://cloud.google.com/compute/docs/sustained-use-discounts">sustained use discounts</a>, transparent pricing with per-second billing, and <a href="https://cloud.google.com/preemptible-vms">preemptible VMs</a> that are discounted up to 80% versus regular instance types.</p><p>Visit our website to get started with <a href="https://cloud.google.com/solutions/hpc">HPC on Google Cloud</a> today.</p></div></div>;https://cloud.google.com/blog/products/compute/how-to-reduce-mpi-latency-for-hpc-workloads-on-google-cloud/
Thu, 03 Dec 2020 23:50:00 -0000;A new Terraform module for serverless load balancing;<div class="block-paragraph"><div class="rich-text"><p>Today we are announcing a new <a href="https://github.com/terraform-google-modules/terraform-google-lb-http/blob/master/modules/serverless_negs/README.md" target="_blank">Terraform module for provisioning load balancers optimized for serverless</a> applications. With this Terraform module, you can complete your load balancing task with a single module, instead of configuring many underlying network resources yourself in Terraform.</p><p><a href="https://cloud.google.com/blog/topics/developers-practitioners/serverless-load-balancing-terraform-hard-way">In an earlier article</a>, we showed you how to build a Cloud HTTPS Load Balancer for your serverless applications from the ground up using Terraform. It was cumbersome, requiring you to know many networking APIs and wire them up. This <a href="https://registry.terraform.io/modules/GoogleCloudPlatform/lb-http/google/latest/submodules/serverless_negs" target="_blank">new Terraform module</a> solves this problem by abstracting away the details of building a load balancer and gives you a single Terraform resource to interact with.</p><p>You can now easily place your serverless applications (Cloud Run, App Engine, or Cloud Functions) behind a Cloud Load Balancer that has an automatic TLS certificate, and lets you set up features like Cloud CDN, Cloud IAP, or custom certificates with only a few lines of configuration.</p><h3>A quick example</h3>For illustration purposes, let’s say you have a <a href="https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts">serverless network endpoint group (NEG)</a> for your Cloud Run application. Provisioning a global HTTPS load balancer for this application on your custom domain with this new Terraform module looks like this:</div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>As you can see from the code snippet above, it takes just a few lines of configuration to set up a managed TLS certificate, deploy your own TLS certificate(s), enable CDN, and define Identity-Aware Proxy or Cloud Armor security policies.</p><h3>Harnessing the power of Terraform</h3><p>With this new Terraform module, you can take deployment automation to the next level by taking advantage of HCL syntax and Terraform features.</p><p>For example, a common use case for using global load balancers is to <a href="https://cloud.google.com/run/docs/multiple-regions">serve traffic from multiple regions</a>. However, automating deployment of your app to <a href="https://cloud.google.com/run/docs/locations">20+ regions</a> and keeping your load balancer configuration up-to-date may require a nontrivial amount of bash scripting. This is where Terraform shines!</p><p>In this <a href="https://github.com/ahmetb/cloud-run-multi-region-terraform/blob/master/main.tf" target="_blank">next example</a>, we deploy a Cloud Run app to all available regions and add them behind a global load balancer. To do that, we simply create a variable named regions that holds the list of deployment locations.</p>Then, we create a Cloud Run service (and its network endpoint group), using the for_each syntax:</div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Then, we add all the network endpoint groups with a simple for loop syntax:</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>By implementing this technique, we create 60+ API resources for 20+ regions with a couple lines of simple modifications to our Terraform configuration, a task that would otherwise require a lot of bash scripting.</p><p>If you’re interested in the full implementation of this example, check out my <a href="https://github.com/ahmetb/cloud-run-multi-region-terraform" target="_blank">Cloud Run multi-region deployment with Terraform</a> repository.</p><h3>Conclusion</h3><p>With the <a href="https://cloud.google.com/load-balancing/docs/negs">serverless network endpoint groups</a> launch, we allow you to create an HTTP or HTTPS load balancer in front of your serverless applications on Google Cloud. And with this new Terraform module, we’re making this interaction even easier.</p><p>To get started, make sure to check out the module <a href="https://github.com/terraform-google-modules/terraform-google-lb-http/blob/master/modules/serverless_negs/README.md" target="_blank">GitHub repository</a> or <a href="https://registry.terraform.io/modules/GoogleCloudPlatform/lb-http/google/latest/submodules/serverless_negs" target="_blank">module registry</a> for the detailed and up-to-date documentation and take a look at the <a href="https://github.com/terraform-google-modules/terraform-google-lb-http/tree/master/examples/cloudrun" target="_blank">Cloud Run example</a> of the module to get started. We welcome your feedback please report <a href="https://github.com/terraform-google-modules/terraform-google-lb-http/issues" target="_blank">issues</a> in our GitHub repository.</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/topics/developers-practitioners/serverless-load-balancing-terraform-hard-way/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Serverless load balancing with Terraform: The hard way</h4><p class="uni-related-article-tout__body">What Cloud Load Balancer integration means for Cloud Run, and how to build a load balancer from scratch using Terraform resources.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/topics/developers-practitioners/new-terraform-module-serverless-load-balancing/
Thu, 03 Dec 2020 19:30:00 -0000;Image archive, analysis, and report generation with Google APIs;<div class="block-paragraph"><div class="rich-text"><p>File backup isn't the most exciting topic, while analyzing images with AI/ML is more interesting—so combining them probably isn't a workflow you think about often. However, by augmenting the former with the latter, you can build a more useful solution than without. Google provides a diverse array of <a href="http://developers.google.com" target="_blank">developer tools</a> you can use to realize this ambition, and in fact, you can craft such a workflow with <a href="http://cloud.google.com">Google Cloud</a> products alone. More compellingly, the basic principle of mixing-and-matching Google technologies can be applied to many other challenges faced by you, your organization, or your customers.</p><br /><p>The sample app presented uses <a href="https://workspace.google.com/products/drive/" target="_blank">Google Drive</a> and <a href="https://workspace.google.com/products/sheets/" target="_blank">Sheets</a> plus <a href="http://cloud.google.com/storage">Cloud Storage</a> and <a href="http://cloud.google.com/vision">Vision</a> to make it happen. The use-case: <a href="http://workspace.google.com" target="_blank">Google Workspace</a> (<a href="https://cloud.google.com/blog/products/workspace/introducing-google-workspace">formerly</a> G Suite) users who work in industries like architecture or advertising, where multimedia files are constantly generated. Every client job results in yet another Drive subfolder and collection of asset files. Successive projects lead to even more files and folders which can lead to users needing to scroll longer to find what they're looking for.</p></div></div><div class="block-paragraph"><div class="rich-text"><p>How can Google Cloud help? Like Drive, Cloud Storage provides file (and generic blob) storage in the cloud. (More on the differences between Drive &amp Cloud Storage can be found in <a href="http://youtu.be/vyIap827rHs" target="_blank">this video</a>.)</p><p>Cloud Storage provides several <a href="http://cloud.google.com/storage/docs/storage-classes">storage classes</a> depending on how often you expect to access your archived files. The <a href="http://cloud.google.com/storage/archival">less often files are accessed</a>, the "colder" the storage, and <a href="https://cloud.google.com/storage/pricing#storage-pricing">the lower the cost</a>. As users progress from one project to another, they're not as likely to need older Drive folders and those make great candidates to backup to Cloud Storage.</p><p>First challenge: determine the security model. When working with Google Cloud APIs, you generally select <a href="https://developers.google.com/identity/protocols/oauth2" target="_blank">OAuth client IDs</a> to access data owned by users and <a href="https://developers.google.com/identity/protocols/oauth2#serviceaccount" target="_blank">service accounts</a> for data owned by applications/projects. The former is typically used with Workspace APIs while the latter is the primary way to access Google Cloud APIs. Since we're using APIs from both product groups, we need to make a decision (for now and change later if desired).</p><p>Since the goal is a simple proof-of-concept, user auth suffices. OAuth client IDs are standard for Drive &amp Sheets API access, and the Vision API only needs API keys so the more-secure OAuth client ID is more than enough. The only IAM permissions to acquire are for the user running the script to get <a href="https://cloud.google.com/storage/docs/access-control/using-iam-permissions">write access to the destination</a> Cloud Storage <a href="http://cloud.google.com/storage/docs/key-terms#buckets">bucket</a>. Lastly, Workspace APIs don't have their own product client libraries (yet), so the lower-level <a href="https://developers.google.com/api-client-library" target="_blank">Google APIs "platform" client libraries</a> serve as a "lowest common denominator" to access all four REST APIs. Those who have written Cloud Storage or Vision code using the <a href="https://cloud.google.com/apis/docs/cloud-client-libraries">Cloud client libraries</a> will see something different.</p><p>The prototype is a command-line script. In real life, it would likely be an application in the cloud, executing as a <a href="http://cloud.google.com/functions">Cloud Function</a> or a <a href="http://cloud.google.com/tasks">Cloud Task</a> running as determined by <a href="http://cloud.google.com/scheduler">Cloud Scheduler</a>. In that case, it would use a service account with Workspace domain-wide delegation to act on behalf of an employee to backup their files. See <a href="http://cloud.google.com/identity/docs/how-to/setup#auth-no-dwd">this page</a> in the documentation describing when you'd use this type of delegation and when not to.</p><p>Our simple prototype targets individual image files, but you can continue to evolve it to support multiple files, movies, folders, and ZIP archives if desired. Each function calls a different API, creating a "service pipeline" with which to process the images. The first pair of functions are <code>drive_get_file()</code> and <code>gcs_blob_upload()</code>. The former queries for the image on Drive, grabs pertinent metadata (filename, ID, MIMEtype, size), downloads the binary "blob" and returns all of that to the caller. The latter uploads the binary along with relevant metadata to Cloud Storage. The script was written in Python for brevity, but the client libraries support <a href="https://developers.google.com/api-client-library" target="_blank">most popular languages</a>. Below is the aforementioned function pseudocode:</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Next, <code>vision_label_img()</code> passes the binary to the Vision API and formats the results. Finally that information along with the file's archived Cloud Storage location are written as a single row of data in a Google Sheet via <code>sheet_append_row()</code>.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Finally, a "main" program that drives the workflow is needed. It comes with a pair of utility functions, <code>_k_ize()</code> to turn file sizes into kilobytes and <code>_linkify()</code> to build a valid Cloud Storage hyperlink as a spreadsheet formula. These are featured here:</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>While this post may feature just pseudocode, a barebones working version can be accomplished with ~80 lines of actual Python. The rest of the code not shown are constants, error-handling, and other auxiliary support. The application gets kicked off with a call to <code>main()</code> passing in a filename, the Cloud Storage bucket to archive it to, a Drive file ID for the Sheet, and a "folder name," e.g., a directory or ZIP archive. Running it several times results in a spreadsheet that looks like this:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Google Sheets" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/google_sheets.max-1000x1000.png" /><figcaption class="article-image__caption "><div class="rich-text">Image archive report in Google Sheets</div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Developers can build this application step-by-step with our "codelab"—codelabs are free, online, self-paced tutorials—which can be found <a href="http://g.co/codelabs/drive-gcs-vision-sheets" target="_blank">here</a>. As you journey through this tutorial, its corresponding <a href="https://github.com/googlecodelabs/analyze_gsimg" target="_blank">open source repo</a> features separate folders for each step so you know what state your app should be in after every implemented function. (NOTE: Files are not deleted, so your users have to decide when to their cleanse Drive folders.) For backwards-compatibility, the script is implemented using <a href="https://github.com/googleapis/oauth2client" target="_blank">older Python auth client libraries</a>, but the repo has an <a href="https://github.com/googlecodelabs/analyze_gsimg/tree/master/alt" target="_blank">"alt" folder</a> featuring alternative versions of the final script that use service accounts, <a href="https://cloud.google.com/apis/docs/cloud-client-libraries">Google Cloud client libraries</a>, and the <a href="https://github.com/googleapis/google-auth-library-python" target="_blank">newer Python auth client libraries</a>. </p><p>Finally to save you some clicks, here are links to the API documentation pages for <a href="http://developers.google.com/drive" target="_blank">Google Drive</a>, <a href="http://cloud.google.com/storage/docs">Cloud Storage</a>, <a href="http://cloud.google.com/vision/docs">Cloud Vision</a>, and <a href="http://developers.google.com/sheets" target="_blank">Google Sheets</a>. While this sample app deals with a constrained resource issue, we hope it inspires you to consider what's possible with Google developer tools so you can build your own solutions to improve users' lives every day!<br /></p></div></div>;https://cloud.google.com/blog/topics/developers-practitioners/image-archive-analysis-and-report-generation-google-apis/
Thu, 03 Dec 2020 17:00:00 -0000;Online shopping gets a boost from Cloud SQL;<div class="block-paragraph"><div class="rich-text"><p><i><b>Editor’s note:</b> With the events of 2020 driving an enormous shift to online shopping, martech provider Bluecore was in prime position. One of our <a href="https://console.cloud.google.com/marketplace/details/bluecore/bluecore">Google Cloud Tech Partners for the Year in Retail</a>, Bluecore offers a marketing platform to over 400 retail brands that combines data and predictive intelligence for targeted campaigns. Here, we look at how Google Cloud SQL’s managed services freed up Bluecore’s valuable time and energy so they could continue to innovate.</i></p><p>At <a href="https://www.bluecore.com/" target="_blank">Bluecore</a>, we help large-scale retail brands transform their shoppers into lifetime customers. We’ve developed a fully automated multi-channel personalized marketing platform that leverages machine learning and artificial intelligence to deliver campaigns through predictive data models. Our product suite includes email, site, and advertising channel solutions, and data is at the heart of everything we do, helping our retailers deliver personalized experiences to their customers. </p><p>Because our retail marketing customers need to access and apply data in real time in their UI—without downtime or a drop in performance—we needed a new database solution. Our engineering team was spending valuable time trying to create and manage our own relational database, which meant less time spent on building our marketing products. We realized we needed a fully managed service that would fit into our existing architecture so that we could focus on what we do best. <a href="https://cloud.google.com/sql">Google Cloud SQL</a> was that solution.</p><h3>Personalized shopping experiences</h3><p>Our retail marketing customers can create highly precise campaigns inside the Bluecore app by applying their marketing and campaign messaging to target customers based on triggers such as referral source, time on page, scroll depth, products browsed, and shopping cart status. Based on those rules, our product intelligently decides which information should be shown to which customers. Highly personalized campaigns can be created easily with drag-and-drop features and widgets such as campaign-specific images or email capture. </p><p>Our requirement for a database was full campaign creation functionality that uses metadata, including type of campaign (pop-up, full-page, etc.), timed campaigns (Christmas, Black Friday, etc.), and targeted customer segments. This campaign metadata needs to be connected and available in real time within the UI itself without slowing down the retail brand’s website. So a marketer’s customer who has a high affinity towards discounts, for example, can be shown products with high discounts when browsing products. </p><p>Once the campaign is rendered, we can measure who engaged with the campaign, what products they browsed, and whether or not they made a purchase. Those analytics are available to the e-commerce marketer and also to our own data science team, so we can measure which campaigns are most effective. We can then use that information to optimize our features and our retail brands’ future campaigns.  </p><p>Using the same underlying data sets and feeds, we can tie the email capabilities to the site capabilities. For instance, if the customer hasn't opened the email in a certain amount of time, and they visit the website, we can show them a campaign. Or if they’ve read a brand’s email, we can show them a different offer. The email and site channels can be used independently or together, according to the marketer’s preference.</p><h3>Needing a real-time solution</h3><p>Our first use case with Cloud SQL was around the storage of campaign information. We have a multi-tenant architecture. Our raw data such as user activity (clicks, views) is stored in raw tables in <a href="https://cloud.google.com/bigquery">BigQuery</a>. At first, our campaign information was stored in <a href="https://cloud.google.com/datastore">Datastore</a>, which can scale easily, but we found out very quickly that our data fits a relational model much better and we started using Cloud SQL.. </p><p>If a marketer makes a change to one campaign, it can affect many other campaigns, so we needed a solution that could take that data and apply it immediately without degraded performance or a need for downtime. This was a mission-critical feature for Bluecore. </p><h3>Choosing Cloud SQL</h3><p>In evaluating relational databases, we looked at a few options, and even tried at first to set up our own MySQL using <a href="https://cloud.google.com/kubernetes-engine">Google Kubernetes Engine</a> (GKE). But we quickly realized that turning to our existing partner, Google, could deliver the results we needed while freeing time for our engineers. Google Cloud SQL had the fully managed database capabilities to provide high availability while handling common time-consuming tasks like backups, maintenance, and replicas. With Google ensuring reliable, secure and scalable databases, our engineers could focus on what we do best, enhancing our marketing platform’s features and performance. </p><p>As an example, one feature that we developed is allowing our retail brand clients the ability to offer custom messaging in real time. For example, we can send a personalized message offering a coupon code in exchange for a customer’s email signup to a customer who has looked at five web pages but hasn’t yet added anything to their cart. </p><h3>Cloud SQL plays well with Google Cloud’s suite of products</h3><p>In addition to our BigQuery and Cloud SQL services, we rely upon many of Google’s related managed services across our infrastructure. Events are being sent from web pages to <a href="https://cloud.google.com/appengine">Google App Engine</a> from which they are queued into <a href="https://cloud.google.com/pubsub">Pub/Sub</a> and processed by Kubernetes/GKE. Our UI is hosted on App Engine as well. It is extremely easy to communicate with Cloud SQL from both App Engine and GKE. Google continues to work with us to realize the full capabilities of the services we use, and to determine which services would best accelerate our growth plan. </p><p>To learn more about their marketing technology platform, visit <a href="https://www.bluecore.com/" target="_blank">Bluecore</a>. Ready to get started with a fully managed relational database solution? Explore <a href="https://cloud.google.com/sql">Cloud SQL</a> now.</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/databases/cloud-database-managed-service-streamlines-data-ops-at-songkick/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Joining fans and artists in perfect harmony with Cloud SQL</h4><p class="uni-related-article-tout__body">Concert discovery service Songkick chose managed cloud database Cloud SQL to modernize their infrastructure and cut out maintenance and o...</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/products/databases/why-bluecore-uses-managed-cloud-databases-from-google-cloud/
Wed, 02 Dec 2020 19:00:00 -0000;Launching code you didn't write: Shipping Next 2020 demos at scale;<div class="block-paragraph"><div class="rich-text"><p>Our biggest conference of the year couldn't happen in person. We've pushed everything online. That's fine, we're Google, we know how to do websites. And demos. And apps. We got this!</p><p>Turns out there were some hiccups along the way. Settle down with your favorite beverage, I'd like to tell you a story.</p><p>Back in the spring, as we were gearing up for <a href="https://cloud.withgoogle.com/next/sf/" target="_blank">Cloud Next 2020</a>, Terry Ryan (@tpryan) and the Cloud Marketing team put their heads together to figure out what a fully online Next conference would be like. Demos are always a big part of Next, giving Google's product teams a chance to show off new products and features to a large audience, in person. These demos are often interactive, frequently eye-catching, and meant to both excite and educate. They include  staffers available to walk attendees through a demo and to explain the complicated bits. A lot to accomplish in person now even harder to move them entirely online!</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Signs hanging at NEXT 2020" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/5A168952-30B1-4837-948C-52B10A6751E4.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Terry took the lead on making this all happen, working across <a href="https://cloud.withgoogle.com/next/sf/demos" target="_blank">27 demos</a> to oversee building, testing and deploying these projects to the NextOnAir site in time for the summer launch. Below we'll talk through some lessons learned from that process, and some of what he built to accomplish this feat.</p><p>Each demo had a different Google team (or an agency) behind it, doing the coding and visual development. Each one their own application, using whatever frontend framework that team is most comfortable with, or best fits the use case. The teams had a lot of flexibility in making their choice: in the end they just needed to deliver a web app that we would host. All demos were served by  App Engine, which we chose because it is easy to use, scales up and down quickly, and provides built-in security.</p><p>Next we head to implementation, where the developer team works to create what the designers envisioned. Here's our first challenge, on the Google side:</p><h3>1. How do we get code from the agencies?</h3><p>In the past all demos focused on delivering a working demo for the event show floor. Formatting and communication could take any form imaginable: an emailed .zip file, a Github repo, a Drive folder, a carrier pigeon. We knew that wouldn't scale. So Terry set up an individual  Github repo for each demo for the agencies to push code to, allowing a single control plane (and logging system) to manage all the demo code. Already a major improvement, and much simpler to manage.</p><p>Additionally, now that every demo is going through Github, we can use some existing Continuous Integration / Continuous Deployment (CICD) tools to automate. Without automation every step of the way Terry would have to manually push each build or each demo to get them published. Not a fun manual chore.. </p><p>Okay, next up, making changes:</p><h3>2. How do we update these demos in production when code changes?</h3><p>Lucky for us we have strong integrations between Cloud Build and Github, so we can trigger a new build each time the master branch is updated in Github (by the team making that demo). We can even automate deploying that new build to App Engine to speed things up.</p><p>All code hosted by Google is required to pass through rigorous security and privacy reviews. This is true in all cases but even more important when the code is created by a third party. However these projects frequently require multiple last minute, urgent updates in response to stakeholder requests. </p><h3>3. How do we control deployment so only reviewed code is pushed?</h3><p>In most cases, third party code has to be pushed to Google hosted sites by a Google employee,  so Terry had to be a bottleneck there. With Github tools, he could prevent unapproved merging of pull requests, set himself up as the required reviewer, then approve the code to trigger a push to the production version  of the App Engine apps, so nothing would get into production without his express authorization. </p><p>With this system in place Terry was able to manage a rapid flurry of updates as the deadlines approached, even if all he had was his phone! But with so much happening simultaneously, there's still another challenge:</p><h3>4. How do we keep these systems moving forward consistently?</h3><p>For that Terry made scripts. So many scripts! He used scripts to activate individual services and service accounts. To bind Cloud Build as an App Engine admin for each project. To perform initial git commits before sharing to Github. To add functionality to Cloud Build pipelines so they could report on whether or not the builds succeeded.</p><p><b>What about securing all these projects and apps?</b></p><p>A script to set up Identity-Aware Proxy for each project, to restrict access to the applications. On the Github side, adding people to each repo and locking down the master branch - Scripts!</p><p>We'll use this to set up the default project and get Cloud Build going.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>[Set the gcloud default project and to kick off a Cloud Build session. ]</p><p>And here we establish some essential APIs we need to use, add some access policy for security, and start the GitHub project with an initial commit.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>[Spin up the APIs we need to run these demos, and initialize the  GitHub  project]</p><p>Next we need to get our email tool so Terry can find out when a build completes, or has an error.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>[Deploy our mail-sending tool in a Cloud Function, to notify on build completion or problems]</p><p>And we're going to add Identity-Aware Proxy for controlling web-app access to only the right people, since we don't want a publicly available IP just yet.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>[Set up and tear down  IAP so only our organization's logged in people can look at these apps]</p><p>Finally we get the directories established, with our template to save time on manual file creation.</p></div></div><div class="block-code"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><pre><code></code></pre></div></div></div></div><div class="block-paragraph"><div class="rich-text"><p>[Script to set up project directories, and copy a template in]</p><p>And then once it's all flowing:</p><h3>5. How do I keep track of all this work?</h3><p>Guess what? It's more scripts. And notifications. Terry made scripts to collect stats on the whole thing, so we know there were 281 commits, 312 code reviews, and <a href="https://cloud.withgoogle.com/next/sf/demos#all" target="_blank">27 demos launched</a>. Setting up the right set of notifications allowed a rapid response time when new code got pushed, and allowed Terry to keep the turnaround tight. When rapidly collaborating with these outside partners that speed becomes critical. These projects averaged 5 minutes from code approval to application pushed out to production. And if the new code push fails, a notification from Cloud Build tells Terry and the development team what went wrong.</p><p>This project brought together many moving parts, many access and authorization challenges, and a tricky form of remote collaboration, but with some clever process design (and a ton of scripting) Terry got it all launched in time for our big show. Congratulations!</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/devops-sre/cloud-build-brings-advanced-cicd-capabilities-to-github/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Cloud Build brings advanced CI/CD capabilities to GitHub</h4><p class="uni-related-article-tout__body">Tighter integration between Cloud Build and GitHub opens up advanced CI/CD workflows for DevOps shops.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/topics/developers-practitioners/launching-code-you-didnt-write-shipping-next-2020-demos-scale/
Wed, 02 Dec 2020 19:00:00 -0000;Cloud Run is now one year old: a look back;<div class="block-paragraph"><div class="rich-text"><p>Cloud Run is built on a simple premise: combine the flexibility of containers with the simplicity, scalability, and productivity of serverless. In a few clicks, you can use Cloud Run to continuously deploy your Git repository to a fully managed environment that autoscales your containers behind a secure HTTPS endpoint. And because Cloud Run is fully managed, there’s no infrastructure management, so you can focus on delivering your applications quickly. </p><p><b>Cloud Run has been generally available (GA) for a full year now!</b> Here's a recap of how Cloud Run has evolved in that time.</p><h3>Enterprise ready</h3><p>We've been hard at work expanding Cloud Run to 21 regions and are on track to be available in all the remaining Google Cloud regions by the end of the year.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Cloud Run to 21 regions.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Run_to_21_regions.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Cloud Run services can now connect to resources with private IPs or use Cloud Memorystore Redis and Memcached by using <a href="https://cloud.google.com/run/docs/configuring/connecting-vpc"><b>Serverless VPC connectors</b></a>, which support <b>shared VPCs</b> so you can connect to resources on-premises or in different projects. By <a href="https://cloud.google.com/run/docs/configuring/static-outbound-ip">routing all egress through the VPC</a>, you benefit from a <b>static outbound IP address</b> for traffic originating from Cloud Run, which can be useful for making calls to external services that only allow certain IP ranges.</p><p>You can now also harness the power of <a href="https://cloud.google.com/blog/products/networking/better-load-balancing-for-app-engine-cloud-run-and-functions"><b>Cloud Load Balancing</b></a> with Cloud Run: bring your own TLS certificates, specify which versions of SSL you accept, or configure custom URL-based routing on your load balancer to serve content from different backends.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Cloud Load Balancing.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Load_Balancing.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Global Load Balancing also enables you to run <b>globally-distributed applications</b> by <a href="https://cloud.google.com/run/docs/multiple-regions">serving traffic from multiple regions</a>, serve static content cached on the edge via <a href="https://cloud.google.com/cdn/docs/setting-up-cdn-with-serverless"><b>Cloud CDN</b></a>, or protect your endpoints with the <a href="https://cloud.google.com/armor"><b>Cloud Armor</b></a> web application firewall.</p><p>With <a href="https://cloud.google.com/blog/products/serverless/google-cloud-api-gateway-is-now-available-in-public-beta"><b>API Gateway</b></a> support, you can build APIs for your customers and run them on Cloud Run without having to implement authentication and other common concerns around hosting APIs.</p><p>With <a href="https://cloud.google.com/blog/products/serverless/cloud-run-now-supports-gradual-rollouts-and-rollbacks"><b>gradual rollouts and rollbacks</b></a>, you can now safely release new revisions of your Cloud Run services by controlling the percentage of traffic sent to each revision, and testing specific revisions behind dedicated URLs.</p><h3>Developer friendly</h3><p>As developers ourselves, we are proud to work on a product that is so well received by the developer community.</p><p>Novice users are able to build and deploy an app on their first try in less than 5 minutes. It's so fast and easy that anyone can deploy multiple times a day. We love hearing your stories about how Cloud Run makes you more productive, so you can ship code faster. <br /></p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Developer friendly.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Developer_friendly.max-1000x1000.jpg" /><figcaption class="article-image__caption "><div class="rich-text"><i>The Cloud Run user interface displays the build history and links to the git repository</i></div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>In the past year, we added a number of features to improve developer productivity:</p><p>We added an easy user interface to set up <a href="https://cloud.google.com/blog/products/application-development/cloud-run-integrates-with-continuous-deployment"><b>Continuous Deployment from Git repositories</b></a>. Every time you push a commit to a particular branch or tag a new release, it’s automatically built and deployed to Cloud Run using Cloud Build.</p><p>We also made it easier to develop applications. You can now <a href="https://cloud.google.com/blog/products/application-development/cloud-run-is-integrated-with-cloud-code">deploy and run Cloud Run applications locally using <b>Cloud Code</b></a>. If you’re just getting started, Cloud Code can create a new application template for you, and you can run or debug your code locally in the emulator.</p><p>Hate writing Dockerfiles? With <a href="https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks"><b>Google Cloud Buildpacks</b></a>, you can now turn your application code directly into a container image for supported languages. This is great if you’re bringing an existing application to Cloud Run. Similarly, buildpacks let you convert your Cloud Functions to container images, thanks to the <a href="https://cloud.google.com/functions/docs/functions-framework"><b>Functions Framework</b></a>.</p><p>Then, to help you monitor the performance of your services and easily identify latency issues, requests sent to Cloud Run services are now captured out-of-the-box in <a href="https://cloud.google.com/trace"><b>Cloud Trace</b></a>. If you want to do distributed tracing between your services, all you need to do is to <a href="https://cloud.google.com/run/docs/logging#writing_structured_logs">pass on the trace header</a> that you get to the outgoing requests and the trace spans will automatically correlate.</p><p>While Cloud Run is able to respond to requests, and to privately and securely process messages pushed by Pub/Sub, we also added the ability to <a href="https://cloud.google.com/blog/products/serverless/build-event-driven-applications-in-cloud-run"><b>trigger Cloud Run services from 60+ Google Cloud sources</b></a>. And for more powerful orchestration, you can leverage <a href="https://cloud.google.com/workflows"><b>Workflows</b></a> to automate and orchestrate processing using Cloud Run.</p><h3>Flexibility</h3><p>We’re constantly pushing the limits of Cloud Run, so you can run more workloads in a fully managed environment. You can now allocate up to <a href="https://cloud.google.com/run/docs/configuring/memory-limits"><b>4GB</b> of memory</a> and <a href="https://cloud.google.com/run/docs/configuring/cpu"><b>4 CPUs</b></a> to your container!</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Flexibility.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Flexibility.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>We also heard you want to minimize "cold starts," notably when scaling from zero. To help, we added the ability to keep a <a href="https://cloud.google.com/run/docs/configuring/min-instances">minimum number of warm container instances</a> to handle requests without hitting the cold starts. These idle instances are priced <a href="https://cloud.google.com/run/pricing#tables">cheaper</a> than active instances when they’re not handling requests. If cold starts are bothering you, give this a try.</p><p><a href="https://cloud.google.com/blog/products/serverless/cloud-run-now-supports-http-grpc-server-streaming"><b>gRPC</b> and <b>server-side streaming</b></a> let you reduce the time-to-first-byte of services by letting you send partial responses as they are computed, and write applications that stream data from the server side. With this capability, you are no longer limited to 32MB responses and you can implement server-sent events (SSE).</p><p>With <a href="https://cloud.google.com/run/docs/configuring/request-timeout"><b>one-hour</b> request timeouts</a>, a single request can now run up to an hour. Combined with server-side streaming, you can now stream large responses (such as documents or videos) or handle long-running tasks from Cloud Run.</p><p>Finally, <a href="https://cloud.google.com/blog/topics/developers-practitioners/graceful-shutdowns-cloud-run-deep-dive"><b>graceful instance termination</b></a> sends your process a termination signal (SIGTERM) before Cloud Run scales down your container instance. This gives you the opportunity to flush out any telemetry data that was kept local, and clean up the open connections, release locks, and so on.</p><h3>What's next for Cloud Run?</h3><p>It was a great first year for Cloud Run, but this is just the beginning. We’re working hard to improve Cloud Run so you can use it for more and more diverse workloads. At the same time, we’re still laser-focused on providing you with a delightful developer experience and addressing key enterprise requirements. Stay tuned for more exciting releases, like mounting secrets from <a href="https://cloud.google.com/secret-manager">Cloud Secret Manager</a>, integration with <a href="https://cloud.google.com/iap">Identity-Aware Proxy</a>, bidirectional streaming and WebSockets.</p><p>To follow past and future Cloud Run features, take a spin through the <a href="https://cloud.google.com/run/docs/release-notes">release notes</a>. And to learn more about these new features, check out <a href="https://youtu.be/os2FPR9WjY0" target="_blank">this video</a>. To help shape the future of Cloud Run, <a href="https://g.co/userresearch/cloudrun" target="_blank">participate in our research studies</a>.</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/serverless/knative-based-cloud-run-services-are-ga/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Cloud Run, a managed Knative service, is GA</h4><p class="uni-related-article-tout__body">Cloud Run, based on Knative, is available on GCP and for Anthos</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/products/serverless/looking-back-on-cloud-runs-first-year-since-ga/
Wed, 02 Dec 2020 17:30:00 -0000;Traffic Director takes application networking beyond Google Cloud;<div class="block-paragraph"><div class="rich-text"><p>Whether you run services in Google Cloud, on-premises, in other clouds, or all of the above, the fundamental challenges of application networking remain the same: How do you get traffic to these services? And how do these services talk to each other?</p><p><a href="https://cloud.google.com/traffic-director">Traffic Director</a> is a fully managed control plane for service mesh and load balancing. We built Traffic Director with the vision that it could solve these challenges, no matter where your services live. Today, we're delivering on another part of that vision so that we can better support your multi-environment needs.</p><p>With Traffic Director, you can now <a href="https://cloud.google.com/traffic-director/docs/multi-environment-overview">send traffic to services and gateways that are hosted outside of Google Cloud</a>. Traffic is routed privately, over <a href="https://cloud.google.com/hybrid-connectivity">Cloud Interconnect</a> or <a href="https://cloud.google.com/network-connectivity/docs/vpn">Cloud VPN</a>, according to the rules that you configure in Traffic Director.</p><p>This capability enables services in your <a href="https://cloud.google.com/vpc/docs/vpc">VPC network</a> to interoperate more seamlessly with services in other environments. It also enables you to build advanced solutions based on Google Cloud's portfolio of networking products, such as Cloud Armor protection for your private on-prem services. Whether you're only running workloads in Google Cloud, or have advanced multi-environment needs, Traffic Director is a versatile tool in your networking toolkit.</p><h3>Introducing Hybrid Connectivity NEGs</h3><p>This is all made possible through support for <i>Hybrid Connectivity</i> <a href="https://cloud.google.com/load-balancing/docs/negs">Network Endpoint Groups</a> (NEGs). Now generally available, think of a Hybrid Connectivity NEG as a collection of IP addresses and ports ("endpoints") that clients can use to reach your application.</p><p>You probably already use NEGs when configuring Traffic Director with GKE-based services. Hybrid Connectivity NEGs are special because they don't need to resolve to a destination within Google Cloud. Instead, they can resolve to a destination outside of your VPC network (like a gateway in an on-prem data center or an application on another public cloud).</p><p>If you're sending traffic from Google Cloud to another environment, that traffic travels over <a href="https://cloud.google.com/hybrid-connectivity">hybrid connectivity</a> (for example, Cloud Interconnect or Cloud VPN). This allows you to have workloads in another environment and reach them securely from Google Cloud, without having to make those workloads accessible via the public internet.</p><p>Here's a simple example of how you might use Hybrid Connectivity NEGs with Traffic Director:</p><ol><li><p>Imagine you have a virtual machine (VM) running on-prem and that VM can be reached from your VPC network via Cloud VPN interconnect at 10.0.0.1 on port 80. </p></li><li><p>In Traffic Director, you create a service called `on-prem-service` and add a Hybrid Connectivity NEG with an endpoint with IP address 10.0.0.1 and port 80. </p></li><li><p>Traffic Director then sends that information to its clients (for example, Envoy sidecar proxies running alongside your applications). Thus, when your application sends a request to `on-prem-service`, the Traffic Director client inspects the request and directs it to `10.0.0.1:80`.</p></li></ol><h3>What can I do with it?</h3><p>With this feature, you can now build powerful solutions that involve existing Traffic Director capabilities as well as <a href="https://cloud.google.com/load-balancing">Cloud Load Balancing</a>'s global network edge services. Here are a few examples:</p><p><b>Route mesh traffic to on-prem or another cloud</b></p><p>The simplest use case for this feature is plain old traffic routing. For example:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><a href="https://storage.googleapis.com/gweb-cloudblog-publish/images/3GoUEQL7yK7NSr5.max-2800x2800.jpg" rel="external" target="_blank"><img alt="plain old traffic routing.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3GoUEQL7yK7NSr5.max-1000x1000.jpg" /></a></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>You want to get traffic from one environment to another. In the above example, when your application sends a request to the `on-prem` service, the Traffic Director client inspects the outbound request and updates its destination. The destination gets set to an endpoint associated with the `on-prem` service (in this case, `10.2.0.1`). The request then travels over VPN or Interconnect to its intended destination.</p><p>If you need to add more endpoints, you just add them to your service by updating Traffic Director. You don't need to make any changes to your application code.</p><p><b>Migrate a service between environments</b></p><p>Being able to send traffic privately to an endpoint outside of Google Cloud is powerful. But things get even more interesting when you combine this with Traffic Director capabilities like weight-based traffic splitting.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><a href="https://storage.googleapis.com/gweb-cloudblog-publish/images/34TnF3iBhArEx4a.max-2800x2800.jpg" rel="external" target="_blank"><img alt="Migrate a service between environments.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/34TnF3iBhArEx4a.max-1000x1000.jpg" /></a></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>The above example extends the previous pattern, but instead of configuring Traffic Director to send all traffic to the `on-prem` service, you configure Traffic Director to split traffic across two services using weight-based traffic splitting.</p><p>Traffic splitting allows you to start by sending 0% of traffic to the `cloud` service and 100% to the `on-prem` service. You can then gradually increase the proportion of traffic sent to the `cloud` service. Eventually, you send 100% of traffic to the `cloud` service and you can retire the `on-prem` service.</p><p><b>Use Google Cloud's edge services with workloads in other environments</b></p><p>Finally, you can combine this functionality with Cloud Load Balancing to bring global edge capabilities to workloads that are outside of your VPC network. Cloud Load Balancing offers a wide range of network edge services, such as globally distributed ingress, <a href="https://cloud.google.com/armor">Cloud Armor</a> for DDoS protection, and <a href="https://cloud.google.com/cdn">Cloud CDN</a>.</p><p>You can now use these with Traffic Director to support workloads that are not exposed to the public internet: if you have Cloud VPN or Cloud Interconnect between Google Cloud and another environment, your traffic will travel via that private route. We previously posted about how you can <a href="https://cloud.google.com/blog/products/identity-security/new-waf-capabilities-in-cloud-armor">use Cloud Armor with on-prem and cloud workloads that can be reached via the public internet</a>. In the approach described below, workloads in other environments don't need to be publicly reachable.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><a href="https://storage.googleapis.com/gweb-cloudblog-publish/images/6fnmwTNNU23utui.max-2800x2800.jpg" rel="external" target="_blank"><img alt="Google Cloud's edge services.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/6fnmwTNNU23utui.max-1000x1000.jpg" /></a></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>In this example, traffic from clients travels over the public internet and enters Google Cloud's network via a Cloud Load Balancer such as our global external HTTP(S) load balancer. You can use network edge services, for example, Cloud Armor DDoS protection or <a href="https://cloud.google.com/iap">Identity-Aware Proxy</a> user authentication when traffic reaches the load balancer.</p><p>After you've secured your ingress path, the traffic makes a brief stop in Google Cloud, where an application or standalone proxy (configured by Traffic Director) forwards it across Cloud VPN or Cloud Interconnect to your on-prem service.</p><h3>Get started today</h3><p>With Traffic Director, we've been focused on enterprise needs since day one, and we're excited about the class of problems that Hybrid Connectivity NEGs solve for enterprises that operate beyond GCP.</p><p><b>Get started today with Traffic Director, Google Cloud Load Balancing and Cloud Armor to</b> <b><a href="https://cloud.google.com/traffic-director/tutorials/network-edge-services-multi-environment">set up a secure global ingress solution for private multi-environment workloads</a>.</b> This is just a first step on our journey to delivering on Traffic Director's mission of being a truly universal application networking solution. More to come!</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/networking/traffic-director-global-traffic-management-for-open-service-mesh/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Google Cloud networking in depth: How Traffic Director provides global load balancing for open service mesh</h4><p class="uni-related-article-tout__body">Google Cloud’s new Traffic Director control-plane management tool brings load balancing a resiliency to environments running on a service...</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/products/networking/traffic-director-extends-its-service-mesh-to-non-gcp-endpoints/
Wed, 02 Dec 2020 17:00:00 -0000;Joining fans and artists in perfect harmony with Cloud SQL;<div class="block-paragraph"><div class="rich-text"><p><i><b>Editor’s note</b>: We’re hearing today from <a href="https://www.songkick.com/" target="_blank">Songkick, a U.K.-based concert discovery service</a> owned by Warner Music Group. Annually, Songkick helps over 175 million music fans from all around the world to track their favorite artists, discover concerts and live streams, and buy tickets with confidence online and via their mobile apps and website. Here’s how the team was able to streamline their process and open up new potential by moving their data from physical servers to the cloud.</i></p><p>Since 2007, we have specialized in making it as easy, fun, and fair as possible for fans to see their favorite artists live. We do this by gathering information shared by artists, promoters, and ticketing partners, storing it on a database of event information, and cross-referencing against user-flagged data in a tracking database. This lets our users know who is playing in their favorite venues, where their favorite artists are performing, and how to get tickets as soon as they’re on sale.</p><p>For many years, all of this depended on physical server space. We managed three racks in an offsite location, so whenever we had any hardware issues, it meant that someone would need to physically go to the location to make changes, even if it was the middle of the night. This meant more unnecessary, time-consuming work for our team and a greater potential for long downtimes. When we were acquired by Warner Music Group, we evaluated what we should focus on and what kind of value we want to deliver as an engineering team. It became clear that maintaining physical machines or database servers were not part of it. </p><h3>Moving to a global venue</h3><p>Moving to the cloud was the obvious solution, and when we did our research, we found that Google Cloud was the best option for us. By adopting Google Cloud managed services, all of our database infrastructure is managed for us, meaning we don’t have to deal with issues like hardware failure—especially not at 4 a.m. It also meant that we no longer had to deal with one of the biggest infrastructure headaches—software upgrades—which, between testing and prep work, previously would have taken over a month to upgrade the physical offsite servers. Honestly, we are just happy to let Google deal with that and our engineers can focus on creating software.</p><p>Migration was thankfully very easy with Google Cloud. Using external replication, we moved one database instance at a time, with about five minutes of downtime for each. We could have made it with almost zero downtime but it was not necessary for our scenario. Today, all four of our databases run on <a href="https://cloud.google.com/sql">Cloud SQL for MySQL</a> with the largest databases—musical event information and artist tour and show tracking information—hosted on dedicated instances. These are quite large our total data usage is around 1.25TB, which includes about 400 GB of event data and 100 GB of tracking data. The two larger databases are 8 CPU, 30 GB of RAM, and the other two are 4 CPU, with 15 GB RAM. We duplicate that data into our staging environment, so total data in CloudSQL is about 2.5 TB.</p><p>Overall, we get to spend less time thinking about and dealing with MySQL, and more time making improvements that directly impact the business.</p><h3>Keeping data clean and clear with Cloud SQL</h3><p>One of the great things about Songkick is that we get data directly from artists, promoters, venues, and ticket sellers, meaning that we can get more accurate information as soon as it’s available. The drawback of this is that when data comes from all of these sources, it means that it comes in multiple formats that often weren’t created to work together. It also means that we often get the same information from multiple sources, which can make things confusing for users.</p><p>Cloud SQL acts as our source-of-truth datastore, ensuring that all of our teams and the 30 applications that contain our business logic are sharing the same information. We apply dedupe and normalization rules on incoming data before it is stored in Cloud SQL, thus reducing the risk of incorrect, inconsistent, duplicated, or incomplete data.</p><p>This is only the beginning of what we’re looking to improve at Songkick on Google Cloud. We’re planning to expand our data processing operations, including creating a service for artists that will show them where their most engaged audiences are, helping them plan better tours. We want to streamline this process by aggregating queries on <a href="https://cloud.google.com/bigquery">BigQuery</a>, then storing the summarized results back in Cloud SQL. That means a better experience for the fans and the artists, and it all starts with a better database in the cloud.</p><p>Learn more about <a href="https://www.songkick.com/" target="_blank">Songkick</a> and about <a href="https://cloud.google.com/products/databases">Google Cloud databases</a>.</p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/databases/cloud-sql-database-service-adds-postgresql-13/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Cloud SQL now supports PostgreSQL 13</h4><p class="uni-related-article-tout__body">Fully managed Cloud SQL cloud database service now supports PostgreSQL 13.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/products/databases/cloud-database-managed-service-streamlines-data-ops-at-songkick/
Wed, 02 Dec 2020 17:00:00 -0000;Simplify creating data pipelines for media with Spotify’s Klio;<div class="block-paragraph"><div class="rich-text"><p>On any given day, music streaming service Spotify might process an audio file a hundred different ways—identifying a track’s rhythm and tempo, timestamping beats, and measuring loudness—as well as more sophisticated processing, such as detecting languages and separating vocals from instruments. This might be done to develop a new feature, to help inform playlists and recommendations, or for pure research.</p><p>Doing this kind of processing on a single audio file is one thing. But Spotify’s music library is over 60 million songs, growing by 40,000 tracks a day, not including the rapidly expanding podcast catalog. Then, factor in that hundreds of product teams are processing these tracks at the same time, all around the world, and for different use cases. This scale and complexity—plus, the difficulty of handling large binary files to begin with—can hinder collaboration and efficiency, bringing product development to a grinding halt. That’s unless you have Klio.</p><h3>What is Klio?</h3><p>In order to productionize audio processing, Spotify created <a href="https://github.com/spotify/klio" target="_blank">Klio</a>—a framework built on top of Apache Beam for Python that helps researchers and engineers alike run large-scale data pipelines for processing audio and other media files (such as video and images). Spotify originally created Klio after realizing that ML and audio researchers across the company were performing similar audio processing tasks, but were struggling to deploy and maintain them. Spotify saw an opportunity to produce a flexible, managed process that would support a variety of audio processing use cases over time—efficiently and at scale—and got to work. </p><p>At a high level, Klio allows a user to provide a media file as input, perform the necessary processing, and output intelligent features and data. There are a multitude of possible use cases for audio alone, from standardizing common audio-processing tasks with <a href="https://ffmpeg.org/" target="_blank">ffmpeg</a> or <a href="https://librosa.org/" target="_blank">librosa</a> to running custom <a href="https://research.atspotify.com/making-sense-of-music-by-extracting-and-analyzing-individual-instruments-in-a-song/" target="_blank">machine learning models</a>. </p><p>Klio simplifies and standardizes pipeline creation for these tasks, increasing efficiency and letting users focus on their business objectives rather than maintaining the processing infrastructure. Now that <a href="https://github.com/spotify/klio" target="_blank">Klio has been released as open source</a>, anyone can use the framework to build their own scalable and efficient media processing workflows.</p><h3>How does Klio work?</h3></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Klio job overview.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Klio_job_overview.max-1000x1000.jpg" /><figcaption class="article-image__caption "><div class="rich-text"><i>Klio job overview</i></div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Klio currently enables a few key steps to create the desired pipeline. First, it assumes that the pipeline will accept a large binary file as input. This can be audio, images, or video. This file is stored in <a href="https://cloud.google.com/storage">Cloud Storage</a>. As part of this, the job sends a unique message to Pub/Sub, where it announces that a file has been uploaded. Klio then reads this message and downloads the file to begin processing. At this step, Klio can begin performing the necessary logic to intelligently process the desired outcome for the particular use case, such as language extraction. Once the processing is complete, it uploads its output artifact to another Cloud Storage bucket for storage. The overall orchestration of the whole pipeline is done by Apache Beam, which allows for a traditional Python interface for audio/ML users and traditional pipeline execution. </p><p>One of Klio’s key benefits is its support for directed acyclic graphs (DAGs), which allow users to configure dependent jobs and their order of execution so that a parent job can trigger corresponding children jobs.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 "><img alt="directed acyclic graphs.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/directed_acyclic_graphs.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>In this example, there are three teams all relying on the same overall parent job, called Downsample. This downsampling adjusts the number of samples in an audio file to essentially compress the file to a specified rate that may be required for later jobs. As a result, now Team A, B, and C’s jobs may begin to launch their needed processing. This might be detecting the “<a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank">speechiness</a>” or amount of spoken word, “<a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank">instrumentalness</a>” or the lack of vocals, and much more.  </p><p>Another key feature of Klio is its ability to optimize the order of execution. It’s not always efficient or necessary to run every Klio job in the graph for a given file. Maybe you want to iterate on your own job without triggering sibling or downstream jobs. Or you have a subset of your media catalogue that requires some backfill processing. Sometimes this means running the parent Klio jobs to fill in missing dependencies. With that, Klio supports bottom-up processing when needed, like this:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 "><img alt="optimize the order of execution.gif" src="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/spotify_gcp_post_bottom_up.gif" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>A Klio job will first check to see if work has already been processed for a given file. If so, work is skipped for that job. However, if the job’s input data is not available (i.e., if the Energy job does not have the output from the Beat Tracking job for a given audio), Klio will recursively trigger jobs within its direct line of execution without triggering work for sibling jobs.</p><h3>What’s next for Klio?</h3><p>This initial release of Klio represents two years of building, testing, and practical application by different teams all across Spotify. From the beginning, Klio was made with open source in mind.</p><p>With this overall architecture, users are free to add in their particular customizations as needed to cater to their requirements. Klio is cloud-agnostic, meaning that it can support a variety of runners, both locally and in the cloud. In Spotify’s case, this meant Google Cloud, using Apache Beam to call the Dataflow Runner. But it can be extended to other runners as well. If you’re interested in <a href="https://docs.klio.io/en/latest/contributors.html" target="_blank">contributing</a> back, they welcome more collaborations with the open source community.  </p><p>While Klio was initially built for audio, it is capable of serving all types of media. At Spotify, they’ve already seen success in a variety of different internal use cases. Specifically, it separates the vocals and instruments to enable <a href="https://research.atspotify.com/making-sense-of-music-by-extracting-and-analyzing-individual-instruments-in-a-song/" target="_blank">Sing Along functionality in Japan</a> as well as fingerprints common audio attributes, such as “danceability” and  “tempo,” in their <a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank">Audio Features API</a>. Based on the early success from these use cases, it will be exciting to see what other media processing problems Klio can help solve, whether it is enabling large-scale content moderation or performing object detection across large video streams.</p><h3>How to get started</h3><p>To learn more, read the rest of the Klio story on the <a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/" target="_blank">Spotify Engineering blog</a>. Or jump in and <a href="https://docs.klio.io/" target="_blank">get started with Klio now</a>.</p></div></div>;https://cloud.google.com/blog/products/data-analytics/try-spotifys-internal-os-tool-for-media-processing-in-beam/
Wed, 02 Dec 2020 15:30:00 -0000;Google enters agreement to acquire Actifio;<div class="block-paragraph"><div class="rich-text"><p>Business continuity is a top priority for enterprise IT organizations, and today we are excited to announce that Google has entered into a definitive agreement to acquire <a href="https://www.actifio.com/" target="_blank">Actifio</a>. </p><p>Actifio is a leader in backup and disaster recovery (DR)—offering customers the opportunity to protect virtual copies of data in their native format, manage these copies throughout their entire lifecycle, and use these copies for scenarios like development and test.</p><p>This planned acquisition further demonstrates Google Cloud’s commitment to helping enterprises protect workloads on-premises and in the cloud. As organizations across industries sharpen their disaster preparedness strategies and infrastructure resiliency, Actifio’s business continuity solutions will help Google Cloud customers prevent data loss and downtime due to external threats, network failures, human errors and other disruptions.  </p><p>"We’re excited to join Google Cloud and build on the success we’ve had as partners over the past four years,” said Ash Ashutosh, CEO at Actifio. “Backup and recovery is essential to enterprise cloud adoption and, together with Google Cloud, we are well-positioned to serve the needs of data-driven customers across industries."</p><p>Actifio helps customers:</p><ul><li><p>Increase business availability by simplifying and accelerating backup and DR at scale, across cloud-native, and hybrid environments. </p></li><li><p>Automatically back up and protect a variety of workloads, including enterprise databases like SAP HANA, Oracle, Microsoft SQL Server, PostgreSQL, and MySQL, as well as virtual machines (VMs) in VMware, Hyper-V, physical servers, and Google Compute Engine.</p></li><li><p>Bring significant efficiencies to data storage, transfer, and recovery. </p></li><li><p>Accelerate application development and reduce DevOps cycles with test data management tools.</p></li></ul><p>“The market for backup and DR services is large and growing, as enterprise customers focus more attention on protecting the value of their data as they accelerate their digital transformations,” said Matt Eastwood, Senior Vice President of Infrastructure Research at IDC. “We think it is a positive move for Google Cloud to increase their focus in this area.”<br /></p><p>We know that customers have many options when it comes to cloud solutions, including backup and DR, and the acquisition of Actifio will help us to better serve enterprises as they deploy and manage business-critical workloads, including in hybrid scenarios. In addition, we are committed to supporting our backup and DR technology and channel partner ecosystem, providing customers with a variety of options so they can choose the solution that best fits their needs.</p></div></div>;https://cloud.google.com/blog/products/storage-data-transfer/google-enters-agreement-to-acquire-actifio/
Wed, 02 Dec 2020 02:00:00 -0000;How computing has evolved, and why you need a multi-cloud strategy;<div class="block-paragraph"><div class="rich-text"><p>Information technology has been moving fast for several years, bringing more powerful and agile computation in the cloud, richer software, better analytics, mobility, and sensors. If only most enterprise technology vendors were keeping up. The incumbents were schooled in the old world of proprietary systems, higher switching costs, and vendor lock-in, and it shows in how they see the world.</p><br /><p>There is no better example of this than in the trend to hybrid- and multi-cloud computing. In both cases, cloud-era technologies provide customers the ability to better use existing assets and take advantage of newer ways to compute, store, and analyze data. This is not theory, but reality. According to <a href="https://www.gartner.com/en/conferences/apac/infrastructure-operations-cloud-india/featured-topics/cloud" target="_blank">Gartner</a>, 81% of organizations are working with two or more public cloud providers. A multi-cloud strategy gives companies the freedom to use the best possible cloud for each workload. </p><br /><p>In contrast, single-cloud stacks impose a significant cost. Where there could be greater power drawn from the unique capabilities of every cloud, there is higher complexity and the limitation of proprietary systems. Where there could be more insight, there is siloed data. Where there could be resilience of entirely different systems, there is concentrated risk. Where there could be more innovation and efficiency, there are impediments. Where there could be a single view of assets, there is a lack of control, haphazard security, and opaque costs.</p><br /><p>At Google Cloud, we’re committed to meeting the needs of customers by providing choice, flexibility and openness. This commitment is reflected in our contributions to projects like Kubernetes, TensorFlow, and <a href="https://opensource.google/" target="_blank">many more</a>.</p><br /><p>Google Cloud is the birthplace of the Kubernetes project. Created by the same engineers that built Kubernetes,<a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a> (GKE) is an easy-to-use cloud-based Kubernetes service for running containerized applications—everywhere, not just on GCP. <a href="https://cloud.google.com/anthos">Anthos</a> builds on the firm foundations of GKE, so you can build out hybrid and multi-cloud deployments with better cloud software production, release, and management—the way you want, not how a vendor dictates. That is key to how a healthy cloud ecosystem works. </p><br /><p>The flexibility to run applications where you need them without added complexity has been a key factor in choosing Anthos—many customers want to continue to use their existing investments both on-premises as well as in other clouds, and having a common management layer helps their teams deliver quality services with low overhead.</p><br /><p>Today, just two years after launch, Anthos now supports more kinds of workloads, in more kinds of environments, in many more locations. According to <a href="https://cloud.google.com/anthos/forrester-tei-report?utm_source=google&amp utm_medium=blog&amp utm_campaign=FY20-Q1-global-demandgen-website-wd-gcp_gtm_anthos_forrestertei">Forrester</a>, Anthos brings a 40% to 55% improvement in platform operating efficiency. Taking multi-cloud even further, recently we <a href="https://cloud.google.com/blog/topics/hybrid-cloud/anthos-on-bare-metal-is-now-ga">announced</a> Anthos on bare metal, so customers could have high performance computing with minimal latency in even remote locations. And the leading API management platform, <a href="https://cloud.google.com/apigee">Apigee</a>, works on every cloud or on-premises, just as it should.</p><br /><p>Anthos is but one part of our commitment to maximize customer power, choice, and control wherever possible. In July we announced <a href="https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-omni">BigQuery Omni</a>, a multi-cloud version of our popular analytics services. For the first time, an enterprise can seamlessly connect directly to their data across Google Cloud, Amazon Web Services (AWS), and (soon) Microsoft Azure, managing large-scale data analysis fast, without having to move or copy data sets, on a single user interface.</p><br /><p>Earlier this year Google Cloud announced the acquisition of <a href="http://www.looker.com" target="_blank">Looker</a>, a multi-cloud data analysis platform that supports multiple data sources and deployment methods. Naturally, Looker as part of Google Cloud still supports hosting on public clouds like AWS, and connects with data sources like Redshift, Snowflake, BigQuery and more than 50 other supported SQL dialects, so you can link to multiple databases, avoid database lock-in, and maintain multi-cloud data environments.</p><br /><p>From open source to multi-cloud to what might be called "analytics anywhere," our strategy is not based on our predetermined need, or some sense of "how it's always been" in enterprise computing, but rather on Google's experience and vision of how computing has evolved, and where it's likely headed. </p><br /><p>Computing wants to be everywhere, you might say, with the right machine crunching the right data for the right purpose. Done right, that's the future: Enabling businesses to innovate and compete wherever they want, using the data they own to best serve their customers with better products and services.</p><br /><p>We’re confident that history is on the side of open-source based multi-cloud APIs. Years ago, open source was condemned, and sometimes forked, to preserve a provider's power over customers. Eventually it was allowed, and today it’s welcomed. Now it's multi-cloud's turn to move from rejection to acceptance and eventually, ubiquity. </p><br /><p>There's a good chance that soon your cloud will do even more of what it should have done in the first place. Watch this space.</p><p><br /></p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/topics/hybrid-cloud/preparing-developers-for-multi-cloud/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">You do you: How to succeed in a distributed, multi-cloud world</h4><p class="uni-related-article-tout__body">How a CIO should prepare developers for a multi-cloud feature.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/topics/hybrid-cloud/future-isnt-just-cloud-its-multi-cloud/
Tue, 01 Dec 2020 18:30:00 -0000;Better service orchestration with Workflows;<div class="block-paragraph"><div class="rich-text"><p>Going from a single monolithic application to a set of small, independent microservices has clear benefits. Microservices enable reusability, make it easier to change and scale apps on demand. At the same time, they introduce new challenges. No longer is there a single monolith with all the business logic neatly contained and services communicating with simple method calls. In the microservices world, communication has to go over the wire with REST or some kind of eventing mechanism and you need to find a way to get independent microservices to work toward a common goal.</p><h3>Orchestration vs Choreography</h3><p>Should there be a central orchestrator controlling all interactions between services or should each service work independently and only interact through events? This is the central question in Orchestration vs Choreography debate. </p><p>In Orchestration, a central service defines and controls the flow of communication between services. With centralization, it becomes easier to change and monitor the flow and apply consistent timeout and error policies. </p><p>In Choreography, each service registers for and emits events as they need. There’s usually a central event broker to pass messages around, but it does not define or direct the flow of communication. This allows services that are truly independent at the expense of less traceable and manageable flow and policies. </p><p>Google Cloud provides services supporting both Orchestration and Choreography approaches. <a href="https://cloud.google.com/pubsub">Pub/Sub</a> and <a href="https://cloud.google.com/blog/products/serverless/build-event-driven-applications-in-cloud-run">Eventarc</a> are both suited for choreography of event-driven services, whereas <a href="https://cloud.google.com/workflows">Workflows</a> is suited for centrally orchestrated services. </p><h3>Workflows: Orchestrator and more</h3></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="Workflows productcard" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/workflows-productcard.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p><a href="http://cloud.google.com/workflows">Workflows</a> is a service to orchestrate not only Google Cloud services, such as Cloud Functions and Cloud Run, but also external services. </p><p>As you might expect from an orchestrator, Workflows allows you to define the flow of your business logic in a <a href="https://cloud.google.com/workflows/docs/reference/syntax">YAML based workflow definition language</a> and provides a <a href="https://cloud.google.com/workflows/docs/reference/executions/rest">Workflows Execution API</a> and Workflows UI to trigger those flows.</p><p>It is more than a mere orchestrator with these built-in and configurable features:</p><ul><li>Flexible retry and error handling between steps for reliable execution of steps.</li><li>JSON parsing and variable passing between steps to avoid glue-code. </li><li>Expression formulas for decisions allow conditional step executions. </li><li>Subworkflows for modular and reusable Workflows.</li><li>Support for external services allows orchestration of services beyond Google Cloud.</li><li>Authentication support for Google Cloud and external services for secure step executions. </li><li>Connectors to Google Cloud services such as Pub/Sub, Firestore, Tasks, Secret Manager for easier integration (in private preview soon). </li></ul><p>Not to mention, Workflows is a fully-managed serverless product. No servers to configure or scale and you only pay for what you use. </p><h2>Use cases</h2><p>Workflows lends itself well to a wide range of use cases. </p><p>For example, in an e-commerce application, you might have a chain of services that need to be executed in a certain order. If any of the steps fail, you want to retry or fail the whole chain. Workflow with its built-in error/retry handling is perfect for this use case:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="pasted image" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/pasted_image_0_1_bfVwbDy.max-1000x1000.png" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>In another application, you might need to execute different chains depending on a condition with Workflow’s conditional step execution:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="workflow2" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Screen_Shot_2020-11-30_at_8.49.15_AM.max-1000x1000.png" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>In long-running batch data processing kind of applications, you usually need to execute many small steps that depend on each other and you want the whole process to complete as a whole. Workflows is well suited because it:</p><ul><li>Supports long-running workflows.</li><li>Supports a variety of Google Cloud compute options such as Compute Engine or GKE for long-running and Cloud Run or Cloud Functions for short-lived data processing.</li><li>Is resilient to system failures. Even if there's a disruption to the execution of the workflow, it will resume at the last check-pointed state.</li></ul><p></p><hr /><p>In orchestration vs choreography debate, there is no right answer. If you’re implementing a well-defined process with a bounded context, something you can picture with a flow diagram, orchestration is often the right solution. If you’re creating a distributed architecture across different domains, choreography can help those systems to work together. You can also have a hybrid approach where orchestrated workflows talk to each other via events. </p><p>I’m definitely excited about using Workflows in my apps and it’ll be interesting to see how people use Workflows with services on Google Cloud and beyond. </p><p>For more information, check out Workflows <a href="https://cloud.google.com/workflows/docs">documentation</a> and feel free to reach out to me on Twitter <a href="https://twitter.com/meteatamel" target="_blank">@meteatame</a>l for questions/feedback!</p><p></p></div></div><div class="block-related_article_tout"><div class="uni-related-article-tout h-c-page"><section class="h-c-grid"><a class="uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker" href="https://gweb-cloudblog-publish.appspot.com/products/gcp/cloud-composer-is-now-in-beta-build-and-run-practical-workflows-with-minimal-effort/"><div class="uni-related-article-tout__inner-wrapper"><p class="uni-related-article-tout__eyebrow h-c-eyebrow">Related Article</p><div class="uni-related-article-tout__content-wrapper"><div class="uni-related-article-tout__image-wrapper"><div class="uni-related-article-tout__image"></div></div><div class="uni-related-article-tout__content"><h4 class="uni-related-article-tout__header h-has-bottom-margin">Cloud Composer is now in beta: build and run practical workflows with minimal effort</h4><p class="uni-related-article-tout__body">Learn more about the beta of Cloud Composer, a managed Apache Airflow service to facilitate your multi-cloud strategy.</p><div class="cta module-cta h-c-copy uni-related-article-tout__cta muted"><span class="nowrap">Read Article<svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"><use xlink:href="#mi-arrow-forward" xmlns:xlink="http://www.w3.org/1999/xlink"></use></svg></span></div></div></div></div></a></section></div></div>;https://cloud.google.com/blog/topics/developers-practitioners/better-service-orchestration-workflows/
Tue, 01 Dec 2020 17:00:00 -0000;Monitor and secure your containers with new Container Threat Detection;<div class="block-paragraph"><div class="rich-text"><p>As more containerized workloads find their way into your organization, you want to be able to detect and respond to threats to containers running in this environment. Today, we’re excited to announce the general availability of Container Threat Detection to help you monitor and secure your container deployments in Google Cloud.</p></div></div><div class="block-paragraph_with_image"><div class="article-module h-c-page"><div class="h-c-grid uni-paragraph-wrap"><div class="uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3"><figure class="article-image--wrap-small "><img alt="container threat detection 3.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/container_threat_detection_3.max-1000x1000.jpg" /></figure><div class="rich-text"><p>Container Threat Detection is a built-in service in <a href="https://cloud.google.com/security-command-center">Security Command Center</a> Premium tier. Container Threat Detection detects the most common container runtime attacks and alerts you to any suspicious activity. This release includes multiple new detection capabilities and provides an API.</p><p>Here are the key findings that are identified by Container Threat Detection:</p><p><b>Suspicious Binary Executions</b>: Container Threat Detection can see when a binary that was not part of the original container image is executed, and triggers a finding, indicating that an attacker may have control of the workload and that they are executing suspicious software such as malware or cryptocurrency mining software.<br /></p><p><b>Suspicious Library Loaded</b>: Container Threat Detection can also detect when a library that was not part of the original container image is loaded—a possible sign that the attacker has control of the workload and that they are executing arbitrary code.<br /></p><p><b>Reverse Shell</b>: Container Threat Detection monitors for processes that get started with stream redirection to a remote connected socket. An attacker can use a reverse shell to communicate from a compromised workload to an attacker controlled machine and perform malicious activities, for example as part of a botnet.<br /></p><h3>Get started today</h3><p>You can get started with Container Threat Detection by simply enabling the built-in service in the Security Command Center with a Premium subscription. To enable a Premium subscription, contact your Google Cloud Platform sales team.</p></div></div></div></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><img alt="container threat detection (1).jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/container_threat_detectio.0394005507850100.max-1000x1000.jpg" /></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>We’ve also made it easy for you to test Container Threat Detection in a non-production environment. To trigger Container Threat Detection findings in a test environment, follow the steps outlined in this <a href="https://cloud.google.com/security-command-center/docs/how-to-test-container-threat-detection">Testing Container Threat Detection guide</a>.</p><p>Security Command Center is a native security and risk management platform for Google Cloud. In addition to Container Threat Detection, it provides built-in services that enables you to gain visibility into your cloud assets, discover misconfigurations and vulnerabilities in your resources, and help maintain compliance based on industry standards and benchmarks.</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><a href="https://storage.googleapis.com/gweb-cloudblog-publish/images/Container_Threat_Detectio.1018027618020533.max-2800x2800.jpg" rel="external" target="_blank"><img alt="Container Threat Detection.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Container_Threat_Detectio.1018027618020533.max-1000x1000.jpg" /></a><figcaption class="article-image__caption "><div class="rich-text"><i>Click to enlarge</i></div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>You can learn more about the Security Command Center and how it can help with your security operations using our <a href="https://cloud.google.com/security-command-center#section-4">product documentation</a>.</p></div></div>;https://cloud.google.com/blog/products/identity-security/container-threat-detection-is-ga/
Tue, 01 Dec 2020 17:00:00 -0000;Best practices to use Apache Ranger on Dataproc;<div class="block-paragraph"><div class="rich-text"><p><a href="https://cloud.google.com/dataproc">Dataproc</a> is an easy-to-use, fully managed cloud service for running managed open source, such as <a href="https://spark.apache.org/" target="_blank">Apache Spark</a>, <a href="https://prestodb.io/" target="_blank">Presto</a>, and <a href="https://hadoop.apache.org/" target="_blank">Apache Hadoop</a> clusters, in a simpler, more cost-efficient way. Dataproc allows you to have long-running clusters similar to always-on on-premises OSS clusters. But even better, it allows multiple smaller, customized, job-focused clusters that can be turned off when a job is done to help manage costs. However, using these ephemeral clusters opens a few questions: How do you manage secure and fine-grained access to Hadoop services in this new architecture? How can you audit user actions and make sure the logs persist beyond any cluster lifecycle?</p><p>In this blog, we propose an end-to-end architecture and best practices to answer these questions using <a href="https://ranger.apache.org/" target="_blank">Apache Ranger</a>, an authorization OSS for Hadoop, on Google Cloud.</p><p>In this architecture, several Dataproc clusters share a single Ranger back-end database while each cluster has its own Ranger admin and plugin components. The database, hosted on <a href="https://cloud.google.com/sql">Cloud SQL</a>, centralizes the policies so that policies are synchronized among all the clusters. </p><p>With this architecture, you don’t have to deploy one Ranger database per cluster and consequently, deal with policy synchronization and incur higher costs. Moreover, you don’t need a central Ranger admin instance, which requires maintenance to be always up. Instead, the only centralized component is your Ranger database, backed by <a href="https://cloud.google.com/sql">Cloud SQL</a>, Google Cloud’s fully managed relational database service.</p><h3>How is the cloud different?</h3><p>With Dataproc you create clusters in a few minutes, manage them easily, and save money by turning clusters off when you don't need them. You can create as many clusters as you need, tailor them for a job or a group of jobs, and have them around only while those jobs are running. </p><p>That sounds great, but how is authentication and authorization managed in such an environment? Dataproc shares the <a href="https://cloud.google.com/dataproc/docs/concepts/iam/iam#permissions">Cloud Identity and Access Management (Cloud IAM)</a> functionalities with the rest of Google Cloud however, <a href="https://cloud.google.com/dataproc/docs/concepts/iam/granular-iam">IAM permissions</a> are high-level and not specifically aimed to control very fine-grained access to the services in a Hadoop environment. That is where Ranger excels. </p><p>If you are used to Ranger on your on-prem environments, you will feel at home on Dataproc. Dataproc supports Ranger as an <a href="https://cloud.google.com/dataproc/docs/concepts/components/ranger">optional component</a>, so you continue to have Ranger installed on each cluster using Dataproc’s component exchange.</p><p>In this diagram, you can see four Dataproc clusters on Google Cloud. Each cluster hosts an instance of Ranger to control access to cluster services such as Hive, Presto, HBase, and others</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><a href="https://storage.googleapis.com/gweb-cloudblog-publish/images/highLevel-01.max-2800x2800.jpg" rel="external" target="_blank"><img alt="four Dataproc clusters.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/highLevel-01.max-1000x1000.jpg" /></a><figcaption class="article-image__caption "><div class="rich-text"><i>Click to enlarge</i></div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Users of these services have their identities defined in an identity provider service that is external to the clusters. As an example, the diagram shows an LDAP server such as <a href="https://directory.apache.org/apacheds/" target="_blank">Apache DS</a> running on Google Compute Engine. However, you can also use your own identity provider like Active Directory on-prem or on a different cloud provider. See <a href="https://cloud.google.com/solutions/authenticating-corporate-users-in-a-hybrid-environment">Authenticating corporate users in a hybrid environment.</a> </p><p>The access policies defined in Ranger are also external to the clusters. The diagram shows them stored in a centralized <a href="https://cloud.google.com/sql">Cloud SQL</a> instance, along with the Ranger <a href="https://cwiki.apache.org/confluence/display/RANGER/Apache+Ranger+0.5+-+User+Guide#ApacheRanger0.5UserGuide-Logintothesystem:" target="_blank">internal users</a>. Finally, auditing is externalized to <a href="https://cloud.google.com/storage">Cloud Storage</a>, with each cluster storing its logs in its own bucket and folder. Having the policies, internal users, and logs separated from the Hadoop clusters allows you to create and turn off clusters as needed.</p><p><b>What is behind the scenes in a cluster?</b></p><p>Let's go under the hood of a cluster and drill down to the components that make this architecture possible:</p></div></div><div class="block-image_full_width"><div class="article-module h-c-page"><div class="h-c-grid"><figure class="article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 "><a href="https://storage.googleapis.com/gweb-cloudblog-publish/images/lowLevel-01.max-2800x2800.jpg" rel="external" target="_blank"><img alt="cluster and drill down.jpg" src="https://storage.googleapis.com/gweb-cloudblog-publish/images/lowLevel-01.max-1000x1000.jpg" /></a><figcaption class="article-image__caption "><div class="rich-text"><i>Click to enlarge</i></div></figcaption></figure></div></div></div><div class="block-paragraph"><div class="rich-text"><p>Users of the system, shown on top of the diagram, want to access one or more of the cluster services to process some data and get results back. </p><p>They authenticate using an <a href="https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security#enabling_hadoop_secure_mode_via_kerberos">on-cluster Kerberos Distribution Center</a>, or alternatively using an Apache Knox Gateway as described in <a href="https://medium.com/google-cloud/connecting-your-visualization-software-to-hadoop-on-google-cloud-64b55f536fab" target="_blank">this article</a>. Both Kerberos and Apache Knox can verify the user identities defined in an external LDAP server. The Ranger User Sync Server periodically retrieves the identities from the LDAP server so that it can apply access policies to the users. </p><p>Dataproc supports <a href="https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/optional-components#kerberos">Kerberos integration on the cluster</a> out of the box. If you use Kerberos in your cluster with this architecture, you need to use an LDAP server as an external <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system-level_authentication_guide/using_trusts" target="_blank">cross-realm trust</a> to map users and groups into Kerberos principals.</p><p>Once a user is authenticated, their request is routed to the appropriate service. However, it is intercepted by the corresponding Ranger plugin for the service. The plugin periodically retrieves the policies from the <a href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_security/content/ad_integration_ranger_architecture.html" target="_blank">Ranger Policy Server</a>. These policies determine if the user identity is allowed to perform the requested action on the specific service. If it is, then the plugin allows the service to process the request and the user gets back the results. Note that the policies are external to the cluster and stored in a Cloud SQL database so that they persist independently of the cluster lifecycle.</p><p>Every user interaction with a Hadoop service, both allowed or denied, is written to cluster logs by the Ranger Audit Server. Each cluster has its own logs folder in Cloud Storage. Ranger can index and search these logs leveraging <a href="https://cloud.google.com/dataproc/docs/concepts/components/solr">Apache Solr</a>. Examining the logs of a previously deleted cluster is as easy as creating a new cluster and pointing the dataproc:solr.gcs.path property to the old cluster logs folder.</p><p>Last but not least, the Admin UI of Ranger is installed to allow an easy way to visualize and manage the different policies, roles, identities, and logs across clusters. Access to the Admin UI is given to a separate group of users, internal to Ranger, and stored in the Ranger database.</p><p>All the Ranger components run on the Hadoop master node. Workers that ultimately run jobs orchestrated through YARN are not pictured in the diagram, and do not need any particular configuration.</p><h3>How does the architecture work with ephemeral clusters? </h3><p>Dataproc allows you to run <a href="https://cloud.google.com/blog/products/data-analytics/10-tips-for-building-long-running-clusters-using-cloud-dataproc">multiple long-running and/or ephemeral clusters simultaneously</a>. Should you install Ranger in every cluster? The answer is yes and no. </p><p>If every cluster had its own Ranger admin and database, it would be cumbersome to re-populate the users and policies every time you have a new cluster. On the other hand, a central Ranger service brings up scalability issues, since it has to deal with the user sync, policy sync, and the audit logs for all the clusters.</p><p>The proposed architecture keeps a central Cloud SQL database always up while all the clusters can be ephemeral. The database stores policies, users, and roles. Every cluster has its own Ranger components synchronized with this database. The advantage of this architecture is that you avoid policy synchronization and the only centralized component is Cloud SQL, which is managed by Google Cloud. See the first figure above that shows the architecture.</p><h3>How do you authenticate users?</h3><p>For Ranger, there are two user types: </p><p><b>External users</b>: These are users that access data processing services such as Hive. In most cases, they do not need explicit access to the Ranger UI. Ranger runs a user synchronization daemon service in every cluster to fetch these users and groups from LDAP, then persists them in the Ranger database. This daemon can run safely in each Dataproc cluster as long as they all fetch users from the same LDAP server with the same parameters. To avoid race conditions, where a particular user is synchronized twice by different clusters, the Ranger database has a uniqueness constraint on user/group IDs.  </p><p><b>Internal users</b>: These are the users of the Ranger UI. Authentication is different from external users. You define authentication to the UI via an LDAP/AD setup or by manually creating the users. This method must be set up in every cluster explicitly because every UI checks its own configuration to learn where to query for authentication. When you create a user via UI directly, Ranger persists that user into the shared database. Hence, it is available in the Ranger UIs on all clusters without any additional configuration.</p><p><b>A Ranger admin user</b> is a special internal user who has the authority to perform any action on the Ranger UI, such as creating policies, adding internal users, and assigning the admin role to others. The Dataproc Ranger component allows you to set the Ranger admin user password during startup and stores the credentials in the central Ranger database. Therefore, the admin user and password are the same across all the clusters.</p><h3>How do you synchronize authorization policies across clusters?</h3><p>Ranger stores authorization policies in a relational database. The architecture uses a shared Cloud SQL Ranger database so that policies are available to all clusters. Admin users can alter these policies by logging into any Ranger UI that shares the same database.</p><h3>How do you audit user actions?</h3><p><a href="https://lucene.apache.org/solr/" target="_blank">Apache Solr</a> handles the Ranger audit logs and stores them in a Cloud Storage bucket for durability even after cluster deletion. </p><p>When you need to read the logs of a deleted cluster, you create a cluster and point Solr to the same Cloud Storage folder. You will then be able to browse the logs in the Ranger UI of that cluster. The cluster that you create for log retrieval can be small, such as a <a href="https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/single-node-clusters">single node cluster</a>, and ephemeral. </p><p>To avoid having different Cloud Storage buckets per cluster, use the same bucket for all as long as each cluster logs to a different folder. Clusters cannot write their audit logs to the same folder since each cluster has its own Solr component managing these logs.</p><p>In addition to Ranger audit logs, Google Cloud provides <a href="https://cloud.google.com/logging/docs/audit">Cloud Audit Logs</a>. These logs are not as granular as the Ranger logs, but are an excellent tool that allows you to answer the questions of "who did what, where, and when?" on your Google Cloud resources. For example, if you use the <a href="https://cloud.google.com/dataproc/docs/guides/submit-job">Dataproc Jobs API</a>, you could find out which Cloud IAM user submitted a job through Cloud Audit Logging. Or you can track the Dataproc Service Account reads and writes on a Cloud Storage Bucket.</p><h3>Use the right access control for your use case</h3><p>Before we finish, we’d ask you to consider whether you need Ranger. Ranger adds minutes to cluster creation and you have to manage its policies. </p><p>As an alternative, you can create many ephemeral Dataproc clusters and assign them individual service accounts with different access rights. Depending on your company size, creating a service account and cluster per person may not be cost-effective, but creating shared clusters per team would offer enough degree of separation for many use cases. </p><p>You can also use <a href="https://cloud.google.com/dataproc/docs/concepts/iam/personal-auth">Dataproc Personal Cluster Authentication</a> if a cluster is only intended for interactive jobs run by an individual (human) user.</p><p>Use these alternatives instead of Ranger when you don't need fine-grained authorization and audit at the service, table, or column level. You can limit a service account or user account to access only a specific cluster and data set.</p><h3>Get started with Ranger on Dataproc</h3><p>In this blog post, we propose a Ranger architecture to serve multiple long-running and/or ephemeral Dataproc clusters. The core idea is sharing the Ranger database, authentication provider, and audit log storage and running all other components such as Ranger Admin, Ranger UI, Ranger User Sync, and Solr in individual clusters. The database serves the policies, users, and their roles for all the clusters. You don’t need to run a central Ranger service because Ranger components are stateless. Solr stores the audit logs on Cloud Storage to keep them for further analysis even after the deletion of a cluster.</p><p>Try Ranger on Dataproc with <a href="https://cloud.google.com/dataproc/docs/concepts/components/ranger">the Dataproc Ranger Component</a> for easy installation. Combine it with <a href="https://cloud.google.com/sql">Cloud SQL</a> as the shared Ranger database. Go one step further and <a href="https://medium.com/google-cloud/connecting-your-visualization-software-to-hadoop-on-google-cloud-64b55f536fab" target="_blank">connect your Visualization Software to Hadoop on Google Cloud</a>.</p></div></div>;https://cloud.google.com/blog/products/data-analytics/running-cloud-managed-spark-and-hadoop-using-ranger/